{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and Uploading MaStR - Winddata with open-mastr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_mastr import Mastr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import supabase_py\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_list = [\"wind\", \"location\", \"permit\"]\n",
    "db = Mastr()\n",
    "#db.download(method = \"bulk\",\n",
    "#            data=tables_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inspect db\n",
    "conn = db.engine # Connection engine\n",
    "tables = pd.read_sql_query('SELECT name from sqlite_master where type= \"table\";', conn)\n",
    "df_wind = pd.read_sql_table(\"wind_extended\", conn)\n",
    "columns_wind = list(df_wind.columns)\n",
    "del conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Very few plants actually have an address -> Not useful for displaying?\n",
    "df_address = df_wind[[\"Strasse\", \"StrasseNichtGefunden\",\n",
    "       \"Hausnummer\", \"HausnummerNichtGefunden\", \"Adresszusatz\"]]\n",
    "df_address.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_remove = [\"Lage\",\t\"Seelage\",\t\"ClusterOstsee\", \"ClusterNordsee\", \"Strasse\", \"StrasseNichtGefunden\",\n",
    "       \"Hausnummer\", \"HausnummerNichtGefunden\", \"Adresszusatz\", \"NetzbetreiberpruefungStatus\", \n",
    "       \"NetzbetreiberpruefungDatum\", \"Wassertiefe\", \"Kuestenentfernung\", \"UtmZonenwert\",\"UtmEast\", \"UtmNorth\",\n",
    " \"GaussKruegerHoch\", \"GaussKruegerRechts\", \"DatenQuelle\", \"DatumDownload\"]\n",
    "columns_wind = [col for col in columns_wind if col not in columns_remove]\n",
    "df_wind = df_wind[columns_wind]\n",
    "columns_wind = df_wind.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to turn CamelCase to snake_case\n",
    "def change_case(str):\n",
    "    # List comprehension, starts with an _ wich is removed by lstrip(\"_\")\n",
    "    # loops through word, if upper, _ first \"_\"+\n",
    "    # and i.lower() as a string method\n",
    "    # just return i else\n",
    "    return ''.join(['_'+i.lower() if i.isupper() \n",
    "               else i for i in str]).lstrip('_')\n",
    "    \n",
    "def dtype_sqltype(str, map_dict):\n",
    "    \n",
    "    # next() jumps through the iterator until a match is found\n",
    "    # with a an iterator generated by the comprehension inside ()\n",
    "    return next((key for key, val in map_dict.items() if val == str), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set schema_name and table_name\n",
    "schema_name = \"public\"\n",
    "table_name = \"wind_extended\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SQL Create Statement\n",
    "pd_types = [df_wind[col].dtype for col in columns_wind]\n",
    "\n",
    "# dictionary mapping the data types: Postgres Data type = pandas data type\n",
    "map_types = {'bool': 'bool', \n",
    "                 'float8': 'float64', \n",
    "                 'date' : '<M8[ns]', \n",
    "                 'varchar':'O'}\n",
    "\n",
    "# column-names for coordinates\n",
    "geo_columns = ['Laengengrad','Breitengrad']\n",
    "\n",
    "sql_columns = []\n",
    "for col in columns_wind:\n",
    "    if col not in geo_columns:\n",
    "    # SQL create column statement for this column: 'column_name pgsql-type,'. Leave out constraints for now\n",
    "    # to lowercase, underscore at uppercase\n",
    "        name = change_case(col)\n",
    "        sql_type = dtype_sqltype(df_wind[col].dtype, map_types)\n",
    "        sql_columns.append(f\"{name} {sql_type}\")\n",
    "\n",
    "sql_columns = \", \\n \".join(sql_columns)\n",
    "\n",
    "# leave out geo-columns -> added later on\n",
    "# add primary key\n",
    "sql_drop = f\"DROP TABLE IF EXISTS {schema_name}.{table_name};\"\n",
    "\n",
    "sql_create = f\"\"\"\n",
    "CREATE TABLE {schema_name}.{table_name} (\n",
    "id bigint generated by default as identity primary key,\n",
    "{sql_columns},\n",
    "geom geometry(point, 4326) \n",
    ");\"\"\" \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to the database\n",
    "# downloaded certiticate\n",
    "# Set connection details in .env\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get connection parameters from environment variables\n",
    "dbname = os.getenv(\"DB_NAME\")\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")\n",
    "ssl_cert_path = os.getenv(\"SSL_CERT_PATH\")\n",
    "\n",
    "# Construct the connection string\n",
    "conn_str = f\"dbname={dbname} user={user} password={password} host={host} port={port} sslmode=require sslrootcert={ssl_cert_path}\"\n",
    "\n",
    "# Etablish connection object\n",
    "\n",
    "#conn.close()\n",
    "conn = psycopg2.connect(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(conn)\n",
    "print(\"Connection to the database successful!\")\n",
    "conn_cursor = conn.cursor()\n",
    "\n",
    "# drop table if it already exists\n",
    "conn_cursor.execute(sql_drop)\n",
    "\n",
    "# table creation\n",
    "conn_cursor.execute(sql_create)\n",
    "conn.commit()\n",
    "\n",
    "conn_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selection works fine, as it does necessarily need conn.commit()\n",
    "sql_test = \"SELECT column_name FROM information_schema.columns WHERE table_schema = 'public' AND table_name = 'wind_extended';\"\n",
    "\n",
    "conn_cursor = conn.cursor()\n",
    "conn_cursor.execute(sql_test)\n",
    "results = conn_cursor.fetchall()\n",
    "print(results)\n",
    "conn_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Function for one preprocessed row without geodata \n",
    "### to generate column names and values\n",
    "\n",
    "def row_data_to_sql(row_data, columns_data):\n",
    "    ### Lists for column names as needed for the postgres-table and the values as given to the sql statement\n",
    "    columns_sql = []\n",
    "    values_sql = []\n",
    "\n",
    "    for col in columns_data:\n",
    "        val = row_data[col].values[0]\n",
    "        # print(val)\n",
    "        # print(type(val))\n",
    "        \n",
    "    # Test wether the column holds a value and is not empty\n",
    "        if pd.notna(val):\n",
    "        # add column name\n",
    "            columns_sql.append(change_case(col))\n",
    "        # Apply date to string transformation\n",
    "            if isinstance(val, str):\n",
    "                #print(\"is_str\")\n",
    "                values_sql.append(f\"$${val}$$\")   # add a pair of parentheses to keep for the join\n",
    "            elif is_datetime64_any_dtype(val):\n",
    "                #print(\"is_datetime\")\n",
    "                val = np.datetime_as_string(val, unit=\"D\")\n",
    "                values_sql.append(f\"'{val}'\")\n",
    "            else:\n",
    "                #print(\"is_float_bool\")\n",
    "                values_sql.append(str(val))     # cast to str without adding parentheses\n",
    "    \n",
    "    return columns_sql, values_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper function to turn coordinates (right now EPSG:4326 only)\n",
    "def row_geo_to_sql(row_geo, geo_columns, columns_sql, values_sql, epsg = 4326):   \n",
    "    \n",
    "    lat = geo_columns[0]\n",
    "    lon = geo_columns[1]\n",
    "    ### Append geom and its values to corresponding lists\n",
    "    if pd.notna(row_geo[lon].values[0]) and pd.notna(row_geo[lat].values[0]):\n",
    "        columns_sql.append('geom')\n",
    "        wkt_point = f\"POINT({row_geo[lon].values[0]} {row_geo[lat].values[0]})\"\n",
    "        val_geo = f\"ST_GeomFromText('{wkt_point}',{epsg})\"\n",
    "        values_sql.append(val_geo)\n",
    "    \n",
    "    return columns_sql, values_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper function to construct the INSERT query for one row \n",
    "#   from the columns_sql and values_sql lists\n",
    "\n",
    "def join_insert_sql(columns_sql, values_sql):\n",
    "    # join sql-column names and values respectively into a single string\n",
    "    columns_sql = \", \\n \".join(columns_sql) \n",
    "    values_sql = \", \\n \".join(values_sql)\n",
    "\n",
    "    # Create INSERT-Query for one row\n",
    "    sql_insert = f\"\"\"INSERT INTO {schema_name}.{table_name} (\n",
    "        {columns_sql} )\n",
    "    VALUES (\n",
    "        {values_sql}\n",
    "        );\n",
    "    \"\"\"\n",
    "    \n",
    "    return sql_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for all rows\n",
    "def df_to_sql_insert(df_wind, conn_db):\n",
    "    # join sql-column names and values respectively into a single string\n",
    "    ### Loop through columns\n",
    "    ### How should the geo-insert look like\n",
    "    columns_wind = df_wind.columns\n",
    "\n",
    "    ### column names where each name corresponds to one value (not true for db column geom) \n",
    "    columns_data = [col for col in columns_wind if col not in geo_columns]\n",
    "    \n",
    "    ### List to hold all INSERT Statements\n",
    "    inserts_all = []\n",
    "    \n",
    "    for i in range(len(df_wind)):\n",
    "        row_wind = df_wind.iloc[[i],:]\n",
    "        \n",
    "        row_data = row_wind[columns_data]    \n",
    "        row_geo = row_wind[geo_columns]\n",
    "        \n",
    "        columns_sql, values_sql = row_data_to_sql(row_data, columns_data)\n",
    "        columns_sql, values_sql = row_geo_to_sql(row_geo, geo_columns, columns_sql, values_sql)\n",
    "        insert_sql = join_insert_sql(columns_sql, values_sql)\n",
    "        \n",
    "        inserts_all.append(insert_sql)\n",
    "    \n",
    "    inserts_all_sql = \" \\n \".join(inserts_all)\n",
    "    \n",
    "    # Establish a connection to the database    \n",
    "    try:\n",
    "        # Create a cursor\n",
    "        cur = conn_db.cursor()\n",
    "    \n",
    "        # Execute your SQL statement\n",
    "        cur.execute(inserts_all_sql)\n",
    "    \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle the exception\n",
    "        print(f\"Error: {e}\")\n",
    "        conn.rollback()\n",
    "    \n",
    "    finally:\n",
    "        # Close the cursor and connection\n",
    "        cur.close()\n",
    "        #conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000  # Set the desired batch size\n",
    "total_rows = len(df_wind)\n",
    "\n",
    "for i in range(0, total_rows, batch_size):\n",
    "    df_batch = df_wind[i:i+batch_size]\n",
    "    df_to_sql_insert(df_batch, conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
