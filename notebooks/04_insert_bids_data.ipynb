{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Short) EDA of bids data and Upload to DB\n",
    "\n",
    "- download units and permits\n",
    "- extract Anlagen EEG from open mastr dump\n",
    "- units-table: How do the mastr nrs start? Are units starting with A among the units, as they are in earlier years of the bid-data?\n",
    "- can all units with a Anlagen_Registernr be linked to units of the units-table?\n",
    "- can any columns be omitted? Following Database Design rules, administrative/locational data should not be kept in the the bid data if they can be linked to via foreign key to the units data\n",
    "- If they can be linked, delete the data in the bids-table -> since this will not be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download, Inspect and Upload Permit Data\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import sqlalchemy\n",
    "from open_mastr import Mastr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get Units data\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get connection parameters from environment variables\n",
    "dbname = os.getenv(\"DB_NAME\")\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")\n",
    "ssl_cert_path = os.getenv(\"SSL_CERT_PATH\")\n",
    "\n",
    "# Construct the connection string\n",
    "conn_str = f\"dbname={dbname} user={user} password={password} host={host} port={port} sslmode=require sslrootcert={ssl_cert_path}\"\n",
    "\n",
    "# Etablish connection object\n",
    "\n",
    "#conn.close()\n",
    "conn = psycopg2.connect(conn_str)\n",
    "\n",
    "sql_select = \"SELECT * FROM public.wind_extended;\"\n",
    "df_wind = pd.read_sql(sql_select, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get bid data\n",
    "with open(\"../data/mastr_bids/bids_cleaned_2017_2023.pkl\", mode = \"rb\") as pkl_file:\n",
    "   df_bids_all = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_starting_letters(x):\n",
    "    x_2 = x.fillna('')\n",
    "    series_match = x_2.apply(lambda ser: re.findall(r\"^[A-Za-z]+\", ser))\n",
    "\n",
    "    vals = []\n",
    "    for match_re in series_match.values:\n",
    "        if len(match_re) == 1:\n",
    "            vals.append(match_re[0])\n",
    "\n",
    "    return pd.unique(vals).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How do mastr_nr s in df_wind and Anlagen_Registernr in df_bids_all start?\n",
    "cols_nr = [col for col in df_wind.columns if re.search(\"mastr_nummer\", col)]\n",
    "\n",
    "cols_nr_start = {}\n",
    "\n",
    "### Find all beginning unique Letters in the mastr_nummer columns \n",
    "for col in cols_nr:\n",
    "    cols_nr_start[col] = find_unique_starting_letters(df_wind[col])\n",
    "\n",
    "# List to store tuples, where each tuple represents one row for the long df    \n",
    "rows_list = []\n",
    "\n",
    "# loop through each nr-column\n",
    "for col, values in cols_nr_start.items():\n",
    "    # loop through each unique value of the number column and create tuple with column name in it\n",
    "    for value in values:\n",
    "        rows_list.append((col, value))\n",
    "\n",
    "pd.DataFrame(rows_list, columns = [\"mastr-column\", \"starting-letters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_unique_starting_letters(df_bids_all[\"Register_Anlagennr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrepancy in df_bids identifiers\n",
    "\n",
    "- No foreign key nr in wind_extended start with only A\n",
    "- Try: Can they be linked to the eeg_anlagen table?\n",
    "- Try: When A in df_bids.Anlagen_Registernr is exchanged by SEE -> can it be linked to the units table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try: When A in df_bids.Anlagen_Registernr is exchanged by SEE -> can it be linked to the units table\n",
    "\n",
    "# copy df\n",
    "df_bids_a = df_bids_all.copy()\n",
    "# .str is an acessor used to apply simple string-methods (used on one string object \"foo\") on a series\n",
    "ind = df_bids_a[\"Register_Anlagennr\"].fillna(\"\").str.startswith(\"A\")\n",
    "df_bids_a = df_bids_a[ind]\n",
    "df_bids_a = df_bids_a[['Name des Bieters', \n",
    "       'Landkreis', 'Postleitzahl', 'Gemeinde', 'Gemarkung',\n",
    "       'Flur / Flurstück', 'Register_Anlagennr', 'Gebotsdatum', 'Zuschlags-Nr']]\n",
    "# Create column A -> SEE\n",
    "df_bids_a[\"A_to_SEE\"] = df_bids_a[\"Register_Anlagennr\"].str.replace(\"A\", \"SEE\")\n",
    "# Create column firs three positions -> SEE\n",
    "df_bids_a[\"pos_3_to_SEE\"] = df_bids_a[\"Register_Anlagennr\"].str.replace(r\"^.{3}\", \"SEE\")\n",
    "\n",
    "# Create column A -> SEE\n",
    "df_bids_a[\"A_to_EEG\"] = df_bids_a[\"Register_Anlagennr\"].str.replace(\"A\", \"EEG\")\n",
    "# Create column firs three positions -> SEE\n",
    "df_bids_a[\"pos_3_to_EEG\"] = df_bids_a[\"Register_Anlagennr\"].str.slice_replace(0,3, \"EEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_list = []\n",
    "for key, row in df_wind.iterrows():\n",
    "    rows_list.append((row[\"einheit_mastr_nummer\"], \n",
    "    row[\"einheit_mastr_nummer\"] in df_bids_a[\"A_to_SEE\"].tolist(),\n",
    "    row[\"einheit_mastr_nummer\"] in df_bids_a[\"pos_3_to_SEE\"].tolist(),\n",
    "    row[\"eeg_mastr_nummer\"] in df_bids_a[\"A_to_EEG\"].tolist(),\n",
    "    row[\"eeg_mastr_nummer\"] in df_bids_a[\"pos_3_to_EEG\"].tolist()))\n",
    "\n",
    "df_test = pd.DataFrame(rows_list, columns=[\"einheit_mastr_nr\", \"A_to_SEE\", \"pos_3_to_SEE\", \"A_to_EEG\", \"pos_3_to_EEG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"A_to_SEE\", \"pos_3_to_SEE\", \"A_to_EEG\", \"pos_3_to_EEG\"]:\n",
    "    print(col, df_test[col].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Can the registernr at least be found?\n",
    "# copy df\n",
    "df_bids_see = df_bids_all.copy()\n",
    "# .str is an acessor used to apply simple string-methods (used on one string object \"foo\") on a series\n",
    "ind = df_bids_see[\"Register_Anlagennr\"].fillna(\"\").str.startswith(\"SEE\")\n",
    "df_bids_see = df_bids_see[ind]\n",
    "df_bids_see = df_bids_see[['Name des Bieters', \n",
    "       'Landkreis', 'Postleitzahl', 'Gemeinde', 'Gemarkung',\n",
    "       'Flur / Flurstück', 'Register_Anlagennr', 'Gebotsdatum', 'Zuschlags-Nr']]\n",
    "\n",
    "rows_list = []\n",
    "for key, row in df_wind.iterrows():\n",
    "    rows_list.append((row[\"einheit_mastr_nummer\"], \n",
    "    row[\"einheit_mastr_nummer\"] in df_bids_see[\"Register_Anlagennr\"].tolist()))\n",
    "\n",
    "df_test = pd.DataFrame(rows_list, columns=[\"einheit_mastr_nr\", \"Register_Anlagennr\"])\n",
    "print(df_test[\"Register_Anlagennr\"].sum(), len(df_bids_see), len(df_bids_all))\n",
    "\n",
    "### 2854 unit_mastr_nrs from wind_extended can be found in the 2868 mastr_nrs starting with SEE from df_bids_see of all 4418 bid-units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve the Anlagentable\n",
    "### Data was already downloaded with open_mastr into local sqllite DB\n",
    "db = Mastr()\n",
    "conn = db.engine # Connection engine\n",
    "\n",
    "tables = pd.read_sql_query('SELECT name from sqlite_master where type= \"table\";', conn)\n",
    "df_eeg = pd.read_sql_table(\"wind_eeg\", con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eeg.columns\n",
    "### Interesting columns with possible link to Anlagennr in Bids:\n",
    "\n",
    "df_eeg[['Zuschlagsnummer', 'VerknuepfteEinheit', 'AnlagenschluesselEeg',\n",
    "       'AnlagenkennzifferAnlagenregister',\n",
    "       'AnlagenkennzifferAnlagenregister_nv']]\n",
    "\n",
    "### AnlagenkennzifferAnlagenregister seems interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_bids_a[\"Register_Anlagennr\"].isin(df_eeg[\"AnlagenkennzifferAnlagenregister\"]).sum(), len(df_bids_a))\n",
    "# 529 units in the bids_a table can be linked to the Anlagen-Table via the ANlagennr starting with a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How many can be linked to the eeg table via the Zuschlagsnr\n",
    "len(df_bids_all) # 4418 units\n",
    "### How do they relate to the Zuschlagsdatum in df_bids_all\n",
    "\n",
    "### 1. Can all bids be linked via the Zuschlagsnr? Do all have a Zuschlagsnr?\n",
    "df_bids_all.info() # Zuschlags-Nr 4418 non-null - All have a Zuschlagsnr\n",
    "\n",
    "df_bids_all[\"Zuschlags-Nr\"].isin(df_eeg[\"Zuschlagsnummer\"]).sum() ### 1392 / 4418\n",
    "\n",
    "len(df_eeg.Zuschlagsnummer.unique().tolist()) # 1220 unique Zuschlagsnr in df_eeg\n",
    "\n",
    "len(df_bids_all[\"Zuschlags-Nr\"].unique().tolist()) # 2059 unique Zuschlagsnr in df_bids_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids_all[\"A_SEE_none\"] = df_bids_all[\"Register_Anlagennr\"].str.extract(r\"^([A-Za-z]).*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids_all[\"Zuschlags-Nr\"].apply(lambda x: len(x)).unique() # All Zuschlagsnr of len 11\n",
    "len_zsnr = df_eeg[\"Zuschlagsnummer\"].fillna(\"\").apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_1 = r'^(WIN\\d{2}-\\d{1,2}-\\d{3})$'\n",
    "scheme_1 = \"1_WIN%Y-%M-nr\"\n",
    "pattern_2 = r'^(WIN\\d{2}-\\d{1,2}/\\d{3})$'\n",
    "scheme_2 = \"2_WIN%Y-%M/nr\"\n",
    "\n",
    "def retrieve_nr_scheme(item):\n",
    "    if len(item) == 0:\n",
    "        return \"0 -\"\n",
    "    elif re.match(pattern_1, item):\n",
    "        return scheme_1\n",
    "    elif re.match(pattern_2, item):\n",
    "        return scheme_2\n",
    "    else:\n",
    "        return \"3_X\"\n",
    "\n",
    "df_bids_all.copy()    \n",
    "df_bids_all[\"Zuschlags-Nr\"].fillna(\"\").apply(lambda x: retrieve_nr_scheme(x)) # '1_WIN%Y-%M-nr', '2_WIN%Y-%M/nr'        \n",
    "df_eeg[\"Zuschlagsnummer\"].fillna(\"\").apply(lambda x: retrieve_nr_scheme(x)).unique() # '0 -', '2_WIN%Y-%M/nr', '3_X'\n",
    "\n",
    "# Mismatch in how the Zuschlagsnr are structured\n",
    "# Replace - with / in Zuschlagsnr in df_bids_all -> then compare merging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids_all[\"Zuschlagsnummer\"] = df_bids_all[\"Zuschlags-Nr\"].apply(lambda x: re.sub(r\"-(\\d{3})$\", r\"/\\1\", x))\n",
    "df_bids_all[\"ZS_nr_in_eeg\"] = df_bids_all[\"Zuschlagsnummer\"].isin(df_eeg[\"Zuschlagsnummer\"])\n",
    "df_bids_all[\"Anl_nr_in_eeg\"] = df_bids_all[\"Register_Anlagennr\"].isin(df_eeg[\"AnlagenkennzifferAnlagenregister\"])\n",
    "df_bids_all[\"Mastr_nr_in_eeg\"] = df_bids_all[\"Register_Anlagennr\"].isin(df_eeg[\"VerknuepfteEinheit\"])\n",
    "df_bids_all[\"A_SEE_none\"] = df_bids_all[\"A_SEE_none\"].fillna(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Overview (by Gebotsdatum)\n",
    "\n",
    "# Nr of units, Nr of Zuschlagsnr, Nr of Zuschlagsnr found, Nr of See found, Nr of A found, Nr of Zuschlagsnr and See/A found, Nr of Zuschlagsnr found but not See/A. Nr of See/A found but not Zuschlagsnr\n",
    "group_gebdatum_df_bids_all = df_bids_all.groupby([\"Gebotsdatum\", \"A_SEE_none\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids_sum = pd.DataFrame()\n",
    "\n",
    "for name, df_group in group_gebdatum_df_bids_all:\n",
    "    # Nr of units\n",
    "    n_units = len(df_group)\n",
    "    n_zsnr = len(df_group[\"Zuschlagsnummer\"].unique())\n",
    "    \n",
    "    # n_zsnr_in_eeg = df_group.groupby(\"Zuschlagsnr\")\n",
    "    n_zsnr_in_eeg = len(df_group[df_group[\"ZS_nr_in_eeg\"]][\"Zuschlagsnummer\"].unique())\n",
    "    \n",
    "    n_mastr_nr_in_eeg = df_group[\"Mastr_nr_in_eeg\"].sum()\n",
    "    n_anl_nr_in_eeg = df_group[\"Anl_nr_in_eeg\"].sum()\n",
    "    \n",
    "    n_zsnr_unit_nr_in_eeg = 0\n",
    "    n_only_zsnr_in_eeg = 0\n",
    "    n_only_unit_nr_in_eeg = 0\n",
    "    \n",
    "    for ind, row in df_group.iterrows():\n",
    "        \n",
    "        if name[1] == \"S\":\n",
    "        \n",
    "            if row[\"ZS_nr_in_eeg\"] == True and row[\"Mastr_nr_in_eeg\"] == True:\n",
    "                  n_zsnr_unit_nr_in_eeg += 1\n",
    "            \n",
    "            if row[\"ZS_nr_in_eeg\"] == True and row[\"Mastr_nr_in_eeg\"] == False:\n",
    "                  n_only_zsnr_in_eeg += 1\n",
    "            \n",
    "            if row[\"ZS_nr_in_eeg\"] == False and row[\"Mastr_nr_in_eeg\"] == True:\n",
    "                  n_only_unit_nr_in_eeg += 1\n",
    "        \n",
    "        elif name[1] == \"A\":\n",
    "            if row[\"ZS_nr_in_eeg\"] == True and row[\"Anl_nr_in_eeg\"] == True:\n",
    "                  n_zsnr_unit_nr_in_eeg += 1\n",
    "            \n",
    "            if row[\"ZS_nr_in_eeg\"] == True and row[\"Anl_nr_in_eeg\"] == False:\n",
    "                  n_only_zsnr_in_eeg += 1\n",
    "            \n",
    "            if row[\"ZS_nr_in_eeg\"] == False and row[\"Anl_nr_in_eeg\"] == True:\n",
    "                  n_only_unit_nr_in_eeg += 1\n",
    "    \n",
    "    dict_row = {\"Gebotsdatum\": name[0], \"n_units\": n_units, \"n_zsnr\":n_zsnr, \"type_unit_nr\":name[1], \n",
    "                    \"n_zsnr_in_eeg\":n_zsnr_in_eeg, \"n_mastr_in_eeg\":n_mastr_nr_in_eeg, \"n_anl_nr_in_eeg\":n_anl_nr_in_eeg,\n",
    "                    \"n_zsnr_unit_nr_in_eeg\":n_zsnr_unit_nr_in_eeg,\n",
    "                    \"n_only_zsnr_in_eeg\":n_only_zsnr_in_eeg, \n",
    "                    \"n_only_unit_nr_in_eeg\":n_only_unit_nr_in_eeg}\n",
    "    \n",
    "    if df_bids_sum.empty:\n",
    "          df_bids_sum = pd.DataFrame(dict_row, index = [0])\n",
    "    else:\n",
    "          df_bids_sum = pd.concat([df_bids_sum, pd.DataFrame(dict_row, index = [0])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids_sum.sort_values(by=[\"type_unit_nr\", \"Gebotsdatum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eeg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Learnings from df_bids_sum\n",
    "# Seldomly all bid-nrs of a bid-date can be linked to the eeg-table\n",
    "# Very bad quota linked bid-nrs / all bid-nrs for earliest bids (2017) and latest bids (2022-2023)\n",
    "# Quota can not be enhanced by using the mastr or anlagen-nr\n",
    "\n",
    "### Summary Zuschlagsdaten\n",
    "print(len(df_bids_all))                                                             # 4418 Bids extracted \n",
    "print(len(df_bids_all[\"Zuschlagsnummer\"].unique()))                                 # 2059 Unique bid nrs \n",
    "print(len(df_bids_all[df_bids_all[\"ZS_nr_in_eeg\"]][\"Zuschlagsnummer\"].unique()))    # 1197 Bid nrs in eeg \n",
    "print(df_bids_all[\"ZS_nr_in_eeg\"].sum())                                            # 2381 Units linked to eeg by bid nr\n",
    "\n",
    "print(df_bids_all[\"Anl_nr_in_eeg\"].sum())                                           # 1208 Units linked to eeg by anlagen_nr\n",
    "print(df_bids_all[\"Mastr_nr_in_eeg\"].sum())                                         # 1605 Units linked to eeg by mastr_nr\n",
    "\n",
    "### All entries of df_eeg have an inbetriebnahmedatum -> explains why latest units can from bid date can not be linked (~530 of 862 bid nrs not linked)\n",
    "### 856 units from anlagen_eeg build later than 2017-05-01 which can not be linked via the Zuschlagsnr to df_bids_all. Quite comparable.\n",
    "### Only 70 of those have Zuschlagsnr. But a lot of these have a Scheme of the Zuschlagsnr not matching the (BK6...) Scheme of the Zuschlagsnr in df_bids_all (WIN%Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do the bid-nrs not found look like -> Sort unique by date\n",
    "df_bids_not_eeg = df_bids_all[~df_bids_all[\"ZS_nr_in_eeg\"]]\n",
    "df_bids_not_eeg = df_bids_not_eeg[[\"Gebotsdatum\", \"Zuschlagsnummer\", \"Zuschlags-Nr\"]].drop_duplicates().sort_values(\"Gebotsdatum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eeg_not_bids = df_eeg[~df_eeg[\"Zuschlagsnummer\"].isin(df_bids_all[\"Zuschlagsnummer\"])]\n",
    "later_date = pd.to_datetime(\"2017-05-01\")\n",
    "df_eeg_not_bids = df_eeg_not_bids[df_eeg_not_bids[\"EegInbetriebnahmedatum\"] > later_date]\n",
    "# df_eeg_not_bids[\"Inbetriebmonat\"] = df_eeg_not_bids[\"EegInbetriebnahmedatum\"].dt.strftime(\"%Y-%m\")\n",
    "df_eeg_not_bids = df_eeg_not_bids[[\"EegInbetriebnahmedatum\", \"Zuschlagsnummer\"]].drop_duplicates().sort_values(\"EegInbetriebnahmedatum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Upload Anlagen_EEG table from mastr in goal:100 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Follow the scheme of the mastr insert/create\n",
    "columns_eeg = df_eeg.columns\n",
    "\n",
    "# Meldedatum, Netzbetreiberzuordnungen, Datenquelle, DatumDownload -> have either no info or are remove\n",
    "cols_remove = [\"Meldedatum\", \"Netzbetreiberzuordnungen\", \"DatenQuelle\", \"DatumDownload\"]\n",
    "\n",
    "columns_eeg = [col for col in columns_eeg if col not in cols_remove]\n",
    "df_eeg = df_eeg[columns_eeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set schema_name and table_name\n",
    "schema_name = \"public\"\n",
    "table_name = \"wind_eeg\"\n",
    "\n",
    "### Get python data types\n",
    "pd_types = [df_eeg[col].dtype for col in columns_eeg]\n",
    "\n",
    "# dictionary mapping the data types: Postgres Data type = pandas data type\n",
    "map_types = {'bool': 'bool', \n",
    "                 'float8': 'float64', \n",
    "                 'date' : '<M8[ns]', \n",
    "                 'varchar':'O'}\n",
    "\n",
    "# set the primary and foreign key columns\n",
    "pk_column = \"EegMastrNummer\"\n",
    "fk_column = \"VerknuepfteEinheit\"\n",
    "fk_table = \"wind_extended\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to turn CamelCase to snake_case\n",
    "def change_case(str):\n",
    "    # List comprehension, starts with an _ wich is removed by lstrip(\"_\")\n",
    "    # loops through word, if upper, _ first \"_\"+\n",
    "    # and i.lower() as a string method\n",
    "    # just return i else\n",
    "    ret_str = ''.join(['_'+i.lower() if i.isupper() \n",
    "               else i for i in str]).lstrip('_')\n",
    "    return ret_str.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"__\", \"_\")\n",
    "    \n",
    "def dtype_sqltype(str, map_dict):\n",
    "    \n",
    "    # next() jumps through the iterator until a match is found\n",
    "    # with a an iterator generated by the comprehension inside ()\n",
    "    return next((key for key, val in map_dict.items() if val == str), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_columns = []\n",
    "for col in columns_eeg:\n",
    "    if col not in [pk_column, fk_column]:\n",
    "    # SQL create column statement for this column: 'column_name pgsql-type,'. Leave out constraints for now\n",
    "    # to lowercase, underscore at uppercase\n",
    "        name = change_case(col)\n",
    "        sql_type = dtype_sqltype(df_eeg[col].dtype, map_types)\n",
    "        sql_columns.append(f\"{name} {sql_type}\")\n",
    "        \n",
    "sql_columns = \", \\n \".join(sql_columns)\n",
    "\n",
    "sql_pk = f\"{change_case(pk_column)} {dtype_sqltype(df_eeg[pk_column].dtype, map_types)} PRIMARY KEY\"\n",
    "sql_fk = f\"einheit_mastr_nummer {dtype_sqltype(df_eeg[fk_column].dtype, map_types)} REFERENCES {schema_name}.{fk_table}(einheit_mastr_nummer)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PK-column\n",
    "# leave out geo-columns -> added later on\n",
    "# add primary key\n",
    "sql_drop = f\"DROP TABLE IF EXISTS {schema_name}.{table_name};\"\n",
    "\n",
    "sql_create = f\"\"\"\n",
    "CREATE TABLE {schema_name}.{table_name} (\n",
    "{sql_pk},\n",
    "{sql_fk},\n",
    "{sql_columns} \n",
    ");\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reestablish postgres/supabase connection\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get connection parameters from environment variables\n",
    "dbname = os.getenv(\"DB_NAME\")\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")\n",
    "ssl_cert_path = os.getenv(\"SSL_CERT_PATH\")\n",
    "\n",
    "# Construct the connection string\n",
    "conn_str = f\"dbname={dbname} user={user} password={password} host={host} port={port} sslmode=require sslrootcert={ssl_cert_path}\"\n",
    "\n",
    "# Etablish connection object\n",
    "#conn.close()\n",
    "conn = psycopg2.connect(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conn)\n",
    "conn_cursor = conn.cursor()\n",
    "\n",
    "# drop table if it already exists\n",
    "conn_cursor.execute(sql_drop)\n",
    "\n",
    "# table creation\n",
    "conn_cursor.execute(sql_create)\n",
    "conn.commit()\n",
    "\n",
    "conn_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Function for one preprocessed row without geodata \n",
    "### to generate column names and values\n",
    "\n",
    "def row_data_to_sql(row_data, columns_data):\n",
    "    ### Lists for column names as needed for the postgres-table and the values as given to the sql statement\n",
    "    columns_sql = []\n",
    "    values_sql = []\n",
    "\n",
    "    for col in columns_data:\n",
    "        val = row_data[col].values[0]\n",
    "        # print(val)\n",
    "        # print(type(val))\n",
    "        \n",
    "    # Test wether the column holds a value and is not empty\n",
    "        if pd.notna(val):\n",
    "        # add column name\n",
    "            columns_sql.append(change_case(col))\n",
    "        # Apply date to string transformation\n",
    "            if isinstance(val, str):\n",
    "                #print(\"is_str\")\n",
    "                values_sql.append(f\"$${val}$$\")   # add a pair of parentheses to keep for the join\n",
    "            elif is_datetime64_any_dtype(val):\n",
    "                #print(\"is_datetime\")\n",
    "                val = np.datetime_as_string(val, unit=\"D\")\n",
    "                values_sql.append(f\"'{val}'\")\n",
    "            else:\n",
    "                #print(\"is_float_bool\")\n",
    "                values_sql.append(str(val))     # cast to str without adding parentheses\n",
    "    \n",
    "    return columns_sql, values_sql\n",
    "\n",
    "### helper function to construct the INSERT query for one row \n",
    "#   from the columns_sql and values_sql lists\n",
    "\n",
    "def join_insert_sql(columns_sql, values_sql):\n",
    "    # join sql-column names and values respectively into a single string\n",
    "    columns_sql = \", \\n \".join(columns_sql) \n",
    "    values_sql = \", \\n \".join(values_sql)\n",
    "\n",
    "    # Create INSERT-Query for one row\n",
    "    sql_insert = f\"\"\"INSERT INTO {schema_name}.{table_name} (\n",
    "        {columns_sql} )\n",
    "    VALUES (\n",
    "        {values_sql}\n",
    "        );\n",
    "    \"\"\"\n",
    "    \n",
    "    return sql_insert\n",
    "\n",
    "### Function for all rows\n",
    "def df_to_sql_insert(df_upload, conn_db):\n",
    "    # join sql-column names and values respectively into a single string\n",
    "    ### Loop through columns\n",
    "    ### How should the geo-insert look like\n",
    "    columns_wind = df_upload.columns\n",
    "\n",
    "    ### column names where each name corresponds to one value (not true for db column geom) \n",
    "    columns_data = columns_wind #[col for col in columns_wind if col not in geo_columns]\n",
    "    \n",
    "    ### List to hold all INSERT Statements\n",
    "    inserts_all = []\n",
    "    \n",
    "    for i in range(len(df_upload)):\n",
    "        row_wind = df_upload.iloc[[i],:]\n",
    "        \n",
    "        row_data = row_wind[columns_data]    \n",
    "        #row_geo = row_wind[geo_columns]\n",
    "        \n",
    "        columns_sql, values_sql = row_data_to_sql(row_data, columns_data)\n",
    "        #columns_sql, values_sql = row_geo_to_sql(row_geo, geo_columns, columns_sql, values_sql)\n",
    "        insert_sql = join_insert_sql(columns_sql, values_sql)\n",
    "        \n",
    "        inserts_all.append(insert_sql)\n",
    "    \n",
    "    inserts_all_sql = \" \\n \".join(inserts_all)\n",
    "    \n",
    "        # Establish a connection to the database    \n",
    "    try:\n",
    "        # Create a cursor\n",
    "        cur = conn_db.cursor()\n",
    "    \n",
    "        # Execute your SQL statement\n",
    "        cur.execute(inserts_all_sql)\n",
    "    \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle the exception\n",
    "        print(f\"Error: {e}\")\n",
    "        conn.rollback()\n",
    "    \n",
    "    finally:\n",
    "        # Close the cursor and connection\n",
    "        cur.close()\n",
    "        #conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert Data for eeg table\n",
    "batch_size = 1000  # Set the desired batch size\n",
    "total_rows = len(df_eeg)\n",
    "\n",
    "for i in range(0, total_rows, batch_size):\n",
    "    df_batch = df_eeg[i:i+batch_size]\n",
    "    # rename fk column\n",
    "    df_batch.rename(columns={\"VerknuepfteEinheit\":\"einheit_mastr_nummer\"}, inplace=True)\n",
    "    df_to_sql_insert(df_batch, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zuschlagsnr\n",
    "\n",
    "- a table bids must be created to link wind_eeg and bids:\n",
    "- In both tables zuschlagsnr is neither NOT NULL nor unique - can appear multiple times\n",
    "- More extensive in df_bids_all -> thus create it from df_bids_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check wether the columns ['Name des Bieters', 'Gebots-Nr', 'Gebotsdatum', 'Zuschlagsdatum']\n",
    "### Which hold only one value for each zuschlagsnummer really hold only one value for each nr\n",
    "df_groups = df_bids_all.groupby(\"Zuschlagsnummer\")\n",
    "\n",
    "df_zsnr_sum = pd.DataFrame()\n",
    "\n",
    "for bid_nr, df in df_groups:\n",
    "    row_vals = [bid_nr]\n",
    "    col_names = ['Name des Bieters', 'Gebots-Nr', 'Gebotsdatum',\n",
    "       'Zuschlagsdatum']\n",
    "    for col in col_names:\n",
    "        row_vals.append(len(df[col].unique()))\n",
    "    \n",
    "    col_names.insert(0, \"Zuschlagsnr\")\n",
    "    \n",
    "    df_row = pd.DataFrame([row_vals], columns=col_names)\n",
    "\n",
    "    if df_zsnr_sum.empty:\n",
    "        df_zsnr_sum = df_row\n",
    "    else: \n",
    "        df_zsnr_sum = pd.concat([df_zsnr_sum, df_row],\n",
    "                                ignore_index=True)\n",
    "\n",
    "# Summary\n",
    "df_zsnr_sum[['Name des Bieters', 'Gebots-Nr', 'Gebotsdatum',\n",
    "       'Zuschlagsdatum']].apply(lambda x: len(x.unique()), axis=0)\n",
    "\n",
    "# for all zuschlagsnr there is really only one value in the respective column -> \n",
    "# these can be included in the link table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create link table\n",
    "cols_link = ['Zuschlagsnummer', 'Name des Bieters', 'Gebots-Nr', 'Gebotsdatum',\n",
    "       'Zuschlagsdatum']\n",
    "\n",
    "df_link = df_bids_all[cols_link].drop_duplicates(ignore_index=True)\n",
    "df_link.rename(columns={\"Gebots-Nr\": \"Gebotsnummer\"}, inplace=True)\n",
    "\n",
    "### Set schema_name and table_name\n",
    "schema_name = \"public\"\n",
    "table_name = \"link_wind_bids\"\n",
    "\n",
    "# set the primary and foreign key columns\n",
    "pk_column = \"Zuschlagsnummer\"\n",
    "\n",
    "sql_columns = []\n",
    "for col in df_link.columns:\n",
    "    if col not in [pk_column]:\n",
    "    # SQL create column statement for this column: 'column_name pgsql-type,'. Leave out constraints for now\n",
    "    # to lowercase, underscore at uppercase\n",
    "        name = change_case(col)\n",
    "        sql_type = dtype_sqltype(df_link[col].dtype, map_types)\n",
    "        sql_columns.append(f\"{name} {sql_type}\")\n",
    "        \n",
    "sql_columns = \", \\n \".join(sql_columns)\n",
    "\n",
    "sql_pk = f\"{change_case(pk_column)} {dtype_sqltype(df_link[pk_column].dtype, map_types)} PRIMARY KEY\"\n",
    "\n",
    "### PK-column\n",
    "# leave out geo-columns -> added later on\n",
    "# add primary key\n",
    "sql_drop = f\"DROP TABLE IF EXISTS {schema_name}.{table_name};\"\n",
    "\n",
    "sql_create = f\"\"\"\n",
    "CREATE TABLE {schema_name}.{table_name} (\n",
    "{sql_pk},\n",
    "{sql_columns}\n",
    ");\"\"\" \n",
    "\n",
    "print(conn)\n",
    "conn_cursor = conn.cursor()\n",
    "\n",
    "# drop table if it already exists\n",
    "conn_cursor.execute(sql_drop)\n",
    "\n",
    "# table creation\n",
    "conn_cursor.execute(sql_create)\n",
    "conn.commit()\n",
    "\n",
    "conn_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_sql_insert(df_link, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Create and Upload wind_bid_units table\n",
    "- remove column already present in link table\n",
    "- add foreign keys in eeg table and wind_bid_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One row where the flurnr ended up in the Anlagennr\n",
    "df_bids_all[\"Register_Anlagennr\"][ind] = 'A3144210207303'\n",
    "df_bids_all[\"Flur / Flurstück\"][ind] = \"Flur 7:\" + \" \" + df_bids_all[\"Flur / Flurstück\"][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_keep = ['Zuschlagsnummer','Bundesland',\n",
    " 'Landkreis',\n",
    " 'Postleitzahl',\n",
    " 'Gemeinde',\n",
    " 'Gemarkung',\n",
    " 'Flur / Flurstück',\n",
    " 'Register_Anlagennr']\n",
    "\n",
    "df_bids_upload = df_bids_all[cols_keep]\n",
    "\n",
    "df_bids_upload[\"einheit_mastr_nummer\"] = None\n",
    "df_bids_upload[\"AnlagenkennzifferAnlagenregister\"] = None\n",
    "\n",
    "for ind, row in df_bids_upload.iterrows():\n",
    "    reg_anl_nr = row[\"Register_Anlagennr\"]\n",
    "    \n",
    "    if pd.notna(reg_anl_nr):\n",
    "        if reg_anl_nr.startswith(\"A\"):\n",
    "            df_bids_upload.loc[ind,\"AnlagenkennzifferAnlagenregister\"] = reg_anl_nr\n",
    "        else:\n",
    "            df_bids_upload.loc[ind,\"einheit_mastr_nummer\"] = reg_anl_nr\n",
    "\n",
    "df_bids_upload.drop(columns = [\"Register_Anlagennr\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids_upload.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create link table\n",
    "# df_bids_upload.rename(columns={\"Flur / Flurstück\": \"FlurFlurstück\"}, inplace=True)\n",
    "\n",
    "### Set schema_name and table_name\n",
    "schema_name = \"public\"\n",
    "table_name = \"wind_bids_units\"\n",
    "\n",
    "# set the primary and foreign key columns\n",
    "fk_column = \"Zuschlagsnummer\"\n",
    "fk_table = \"link_wind_bids\"\n",
    "\n",
    "sql_columns = []\n",
    "for col in df_bids_upload.columns:\n",
    "    if col not in [fk_column]:\n",
    "    # SQL create column statement for this column: 'column_name pgsql-type,'. Leave out constraints for now\n",
    "    # to lowercase, underscore at uppercase\n",
    "        name = change_case(col)\n",
    "        sql_type = dtype_sqltype(df_bids_upload[col].dtype, map_types)\n",
    "        sql_columns.append(f\"{name} {sql_type}\")\n",
    "        \n",
    "sql_columns = \", \\n \".join(sql_columns)\n",
    "sql_fk = f\"{change_case(fk_column)} {dtype_sqltype(df_eeg[fk_column].dtype, map_types)} REFERENCES {schema_name}.{fk_table}({change_case(fk_column)})\"\n",
    "\n",
    "### PK-column\n",
    "# leave out geo-columns -> added later on\n",
    "# add primary key\n",
    "sql_drop = f\"DROP TABLE IF EXISTS {schema_name}.{table_name};\"\n",
    "\n",
    "sql_create = f\"\"\"\n",
    "CREATE TABLE {schema_name}.{table_name} (\n",
    "    id bigint generated by default as identity PRIMARY KEY,\n",
    "    {sql_fk},\n",
    "    {sql_columns}\n",
    ");\"\"\"\n",
    "\n",
    "print(conn)\n",
    "print(sql_create)\n",
    "conn_cursor = conn.cursor()\n",
    "\n",
    "# drop table if it already exists\n",
    "conn_cursor.execute(sql_drop)\n",
    "\n",
    "# table creation\n",
    "conn_cursor.execute(sql_create)\n",
    "conn.commit()\n",
    "\n",
    "conn_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids_upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert Data for eeg table\n",
    "batch_size = 1000  # Set the desired batch size\n",
    "total_rows = len(df_bids_upload)\n",
    "\n",
    "for i in range(0, total_rows, batch_size):\n",
    "    df_batch = df_bids_upload[i:i+batch_size]\n",
    "    # rename fk column\n",
    "    df_to_sql_insert(df_batch, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check wether any zuschlagsnummer from wind_eeg are not present in link_wind_bids and Insert those into link_wind_bids\n",
    "### One time directly in sql, otherwise beforehand with python, before wind_eeg is updated -> check if value is present\n",
    "schema_name = \"public\"\n",
    "table_name = \"wind_eeg\"\n",
    "fk_column = \"zuschlagsnummer\"\n",
    "fk_table = \"link_wind_bids\"\n",
    "\n",
    "sql_select = f\"\"\"SELECT DISTINCT {fk_column}\n",
    "FROM {schema_name}.{table_name}\n",
    "WHERE {fk_column} IS NOT NULL\n",
    "  AND {fk_column} NOT IN (SELECT {fk_column} FROM {schema_name}.{fk_table});\"\"\"\n",
    "### And Insert values for gebotsdatum and zuschlagsdatum if WIN\\d{2}-\\d{1,2}/ are present and have a date\n",
    "\n",
    "### Retrieve the values\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "    \n",
    "# Execute your SQL statement\n",
    "cur.execute(sql_select)\n",
    "list_missing_bid_nr = [row[0] for row in cur.fetchall()]\n",
    "    \n",
    "# \n",
    "cur.close()\n",
    "list_missing_bid_nr\n",
    "\n",
    "### The following are present in link bids but where written false in wind_eeg\n",
    "list_correct_bids = [val.replace(\"l\", \"I\") for val in list_missing_bid_nr if re.match(r\"^WlN\", val)]\n",
    "list_correct_bids = list_correct_bids + ['WIN21-3/210', 'WIN21-3/212'] \n",
    "list_correct_bids\n",
    "\n",
    "#['Win21-3/210', 'Win21-3/212', 'Inn20-1/064', 'WIN00-6/797'] \n",
    "# Idea first wln to win, than try rest automatically later\n",
    "list_error_bids = [val for val in list_missing_bid_nr if re.match(r\"^WlN\", val)] + ['Win21-3/210', 'Win21-3/212']\n",
    "\n",
    "df_error_bids = pd.DataFrame({\"correct\":list_correct_bids, \"error\":list_error_bids})\n",
    "# Change these values in wind_eeg\n",
    "# add the res of the values as empty in link_wind_bids\n",
    "\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for _, row in df_error_bids.iterrows():\n",
    "    error_value = row['error']\n",
    "    correct_value = row['correct']\n",
    "\n",
    "    # Update the error value with the correct value in wind_eeg table\n",
    "    sql_update = f\"\"\"\n",
    "    UPDATE {schema_name}.{table_name}\n",
    "    SET {fk_column} = '{correct_value}'\n",
    "    WHERE {fk_column} = '{error_value}';\n",
    "    \"\"\"\n",
    "    cur.execute(sql_update)\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_missing_bid_nr = [val for val in list_missing_bid_nr if val not in list_error_bids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
      "        FROM public.link_wind_bids\n",
      "        WHERE zuschlagsnummer LIKE 'WIN17-2%';\n",
      "[('2017-08-22', '2017-08-01')]\n",
      "SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
      "        FROM public.link_wind_bids\n",
      "        WHERE zuschlagsnummer LIKE 'WIN23-3%';\n",
      "[]\n",
      "SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
      "        FROM public.link_wind_bids\n",
      "        WHERE zuschlagsnummer LIKE 'WIN17-2%';\n",
      "[('2017-08-22', '2017-08-01')]\n",
      "SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
      "        FROM public.link_wind_bids\n",
      "        WHERE zuschlagsnummer LIKE 'WIN17-2%';\n",
      "[('2017-08-22', '2017-08-01')]\n",
      "SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
      "        FROM public.link_wind_bids\n",
      "        WHERE zuschlagsnummer LIKE 'WIN17-2%';\n",
      "[('2017-08-22', '2017-08-01')]\n",
      "SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
      "        FROM public.link_wind_bids\n",
      "        WHERE zuschlagsnummer LIKE 'WIN17-2%';\n",
      "[('2017-08-22', '2017-08-01')]\n",
      "SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
      "        FROM public.link_wind_bids\n",
      "        WHERE zuschlagsnummer LIKE 'WIN17-2%';\n",
      "[('2017-08-22', '2017-08-01')]\n",
      "SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
      "        FROM public.link_wind_bids\n",
      "        WHERE zuschlagsnummer LIKE 'WIN21-2%';\n",
      "[(None, '2021-05-01')]\n",
      "SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
      "        FROM public.link_wind_bids\n",
      "        WHERE zuschlagsnummer LIKE 'WIN17-3%';\n",
      "[('2017-11-22', '2017-11-01')]\n",
      "SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
      "        FROM public.link_wind_bids\n",
      "        WHERE zuschlagsnummer LIKE 'WIN17-1%';\n",
      "[('2017-05-26', '2017-05-01')]\n",
      "SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
      "        FROM public.link_wind_bids\n",
      "        WHERE zuschlagsnummer LIKE 'WIN17-2%';\n",
      "[('2017-08-22', '2017-08-01')]\n",
      "   zuschlagsnummer zuschlagsdatum gebotsdatum\n",
      "0      WIN17-2/065     2017-08-22  2017-08-01\n",
      "1      WIN23-3/018           None        None\n",
      "2      WIN17-2/142     2017-08-22  2017-08-01\n",
      "3    BK6-18-001-15           None        None\n",
      "4      WIN17-2/179     2017-08-22  2017-08-01\n",
      "5      WIN17-2/203     2017-08-22  2017-08-01\n",
      "6      WIN17-2/205     2017-08-22  2017-08-01\n",
      "7      WIN17-2/183     2017-08-22  2017-08-01\n",
      "8    BK6-18-001-04           None        None\n",
      "9      WIN21-2/017           None  2021-05-01\n",
      "10     WIN17-3/132     2017-11-22  2017-11-01\n",
      "11     Inn20-1/064           None        None\n",
      "12     WIN17-1/160     2017-05-26  2017-05-01\n",
      "13     WIN17-2/147     2017-08-22  2017-08-01\n",
      "14     WIN00-6/797           None        None\n"
     ]
    }
   ],
   "source": [
    "# The remaining Zuschlagsnr are not within link_wind_bids (wind_bids_units)\n",
    "# To be able to make wind_eeg.zuschlagsnr a fk-column, list_missing_bid_nr must be written into link_wind_bids\n",
    "# to make it as complete as possible, at least add a date where possible.\n",
    "# The date is queried from similar bid-nrs of the same bid-date\n",
    "df_dates = pd.DataFrame()\n",
    "        # Create a cursor\n",
    "cur = conn.cursor()\n",
    "for val in list_missing_bid_nr:\n",
    "    dict_row = {}\n",
    "    \n",
    "    val_6 = val[0:7]\n",
    "    dict_row['zuschlagsnummer']= val\n",
    "    \n",
    "    if re.match(\"^WIN[12]\\d-\\d\", val_6):\n",
    "        sql_dates = f\"\"\"SELECT DISTINCT zuschlagsdatum, gebotsdatum\n",
    "        FROM {schema_name}.{fk_table}\n",
    "        WHERE {fk_column} LIKE '{val_6}%';\"\"\"\n",
    "        \n",
    "        print(sql_dates)\n",
    "        cur.execute(sql_dates)\n",
    "        res = cur.fetchall()\n",
    "        print(res)\n",
    "        # res to a good row object\n",
    "        if len(res)!=0:\n",
    "            dict_row['zuschlagsdatum']=res[0][0]\n",
    "            dict_row['gebotsdatum']=res[0][1]\n",
    "        else: \n",
    "            dict_row['zuschlagsdatum']=None\n",
    "            dict_row['gebotsdatum']=None\n",
    "            \n",
    "    else: \n",
    "        dict_row['zuschlagsdatum']=None\n",
    "        dict_row['gebotsdatum']=None\n",
    "    \n",
    "    df_row = pd.DataFrame(dict_row, index=[0])\n",
    "    if df_dates.empty:\n",
    "        df_dates = df_row\n",
    "    else: \n",
    "        df_dates = pd.concat([df_dates, df_row], ignore_index=True)\n",
    "\n",
    "print(df_dates)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check schema and links!\n",
    "### Add foreign-Key wind_eeg.zuschlagsnummer -> link_wind_bids.zuschlagsnummer\n",
    "\n",
    "schema_name = \"public\"\n",
    "table_name = \"wind_eeg\"\n",
    "fk_column = \"zuschlagsnummer\"\n",
    "fk_table = \"link_wind_bids\"\n",
    "\n",
    "sql_foreign_key = f\"\"\"\n",
    "    ALTER TABLE {schema_name}.{table_name}\n",
    "    ADD CONSTRAINT foreign_key_{fk_column} \n",
    "    FOREIGN KEY ({fk_column})\n",
    "    REFERENCES {schema_name}.{fk_table} ({fk_column});\n",
    "    \"\"\"\n",
    "\n",
    "try:\n",
    "        # Create a cursor\n",
    "        cur = conn.cursor()\n",
    "    \n",
    "        # Execute your SQL statement\n",
    "        cur.execute(sql_foreign_key)\n",
    "    \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "    \n",
    "except Exception as e:\n",
    "        # Handle the exception\n",
    "        print(f\"Error: {e}\")\n",
    "        conn.rollback()\n",
    "        \n",
    "        # Error: insert or update on table \"wind_eeg\" violates foreign key constraint \"foreign_key_zuschlagsnummer\"\n",
    "        # DETAIL:  Key (zuschlagsnummer)=(WIN17-2/183) is not present in table \"link_wind_bids\".\n",
    "    \n",
    "finally:\n",
    "        # Close the cursor and connection\n",
    "        cur.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
