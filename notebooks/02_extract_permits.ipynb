{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download, Inspect and Upload Permit Data\n",
    "from open_mastr import Mastr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import supabase_py\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "from datetime import datetime\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data was already downloaded with open_mastr into local sqllite DB\n",
    "db = Mastr()\n",
    "conn = db.engine # Connection engine\n",
    "\n",
    "df_permit = pd.read_sql_table(\"permit\", conn)\n",
    "columns_permit = list(df_permit.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to turn CamelCase to snake_case\n",
    "def change_case(str):\n",
    "    # List comprehension, starts with an _ wich is removed by lstrip(\"_\")\n",
    "    # loops through word, if upper, _ first \"_\"+\n",
    "    # and i.lower() as a string method\n",
    "    # just return i else\n",
    "    return ''.join(['_'+i.lower() if i.isupper() \n",
    "               else i for i in str]).lstrip('_')\n",
    "    \n",
    "def dtype_sqltype(str, map_dict):\n",
    "    \n",
    "    # next() jumps through the iterator until a match is found\n",
    "    # with a an iterator generated by the comprehension inside ()\n",
    "    return next((key for key, val in map_dict.items() if val == str), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How many are linked to windpower-data?\n",
    "df_permit.info() # VerknuepfteEinheiten as Foreign-Key column\n",
    "\n",
    "### Open Windpower-Data\n",
    "### Connect to the database\n",
    "# downloaded certiticate\n",
    "# Set connection details in .env\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get connection parameters from environment variables\n",
    "dbname = os.getenv(\"DB_NAME\")\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")\n",
    "ssl_cert_path = os.getenv(\"SSL_CERT_PATH\")\n",
    "\n",
    "# Construct the connection string\n",
    "conn_str = f\"dbname={dbname} user={user} password={password} host={host} port={port} sslmode=require sslrootcert={ssl_cert_path}\"\n",
    "\n",
    "# Etablish connection object\n",
    "\n",
    "#conn.close()\n",
    "conn = psycopg2.connect(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get windpower-Data\n",
    "# Mastr-Einheiten-Nummer\n",
    "# Spalte in permit verknüpfte Einheiten als array\n",
    "# Welche enthalten Wind-Einheiten -> Sind unter diesen jeweils alle eines Arrays bekannte Windeinheiten?\n",
    "\n",
    "# Nur die mit Wind in die Datenbank schreiben, \n",
    "# Genehmigungsnummer als FK zu permits als Spalte in wind_extended hinzufügen\n",
    "# Welche Constraints sind sinnvoll?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_select = \"SELECT * FROM public.wind_extended;\"\n",
    "df_wind = pd.read_sql(sql_select, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One item of df_permit[\"Verknuefte_Einheiten\"]: \"hdjsahd98698, 875875hjhjhj, 65654dhdf\"\n",
    "rows = []\n",
    "\n",
    "for index,row in df_permit.iterrows():\n",
    "    permit = row[\"GenMastrNummer\"]\n",
    "    units = row[\"VerknuepfteEinheiten\"].split(\", \")\n",
    "    \n",
    "    for unit in units:\n",
    "        rows.append({\"GenMastrNummer\": permit, \"Einheit\": unit})\n",
    "\n",
    "df_permits_units = pd.DataFrame(rows)\n",
    "df_permits_units.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_keep = ['id', 'name_windpark','eeg_mastr_nummer',\n",
    "             'einheit_systemstatus',\n",
    "       'einheit_betriebsstatus',      \n",
    "       'einheit_mastr_nummer', \n",
    "       'registrierungsdatum',\n",
    "       'meldedatum', 'geplantes_inbetriebnahmedatum', 'inbetriebnahmedatum',\n",
    "       'datum_endgueltige_stilllegung',\n",
    "       'datum_beginn_voruebergehende_stilllegung',\n",
    "       'datum_beendigung_vorlaeufigen_stilllegung',\n",
    "       'datum_wiederaufnahme_betrieb',\n",
    "       'datum_letzte_aktualisierung']\n",
    "df_wind_perm = df_wind[cols_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_permits_wind = pd.merge(df_permits_units, df_wind_perm, \n",
    "                           left_on=\"Einheit\", right_on=\"einheit_mastr_nummer\",\n",
    "                           how=\"outer\")\n",
    "\n",
    "# Wie viele aus Wind haben eine Genehmigung\n",
    "# Wie ist der Betriebsstatus\n",
    "\n",
    "# Wie viele aus Permit sind in Wind?\n",
    "# Gibt es Dopplungen:\n",
    "# Eine Anlage aus Wind -> mehrere Genehmigungen?\n",
    "# Eine Genehmigung\n",
    "\n",
    "# Sind alle Genehmigungen Unique -> in df_permit überprüfen\n",
    "len(df_permit.GenMastrNummer)           # 30684\n",
    "len(df_permit.GenMastrNummer.unique())  # 30684\n",
    "\n",
    "# Sind bis auf NaN alle Einheiten (.Einheit = alle verknüpften Einheiten in df_permit, .einheit_mastr_nummer = Windkraft-Einheiten)\n",
    "# Unique?\n",
    "ind = df_permits_wind.einheit_mastr_nummer.notna()\n",
    "len(df_permits_wind.einheit_mastr_nummer[ind])           # 35227\n",
    "len(df_permits_wind.einheit_mastr_nummer[ind].unique())  # 35227\n",
    "len(df_wind)                                             # 35227\n",
    "\n",
    "# Wie sind die Genehmigungen verteilt?\n",
    "# df beschränken auf notna() -> ind von oben\n",
    "df_permits_wind = df_permits_wind.loc[ind,:]\n",
    "\n",
    "# Haben alle eine Einheiten eine Genehmigung\n",
    "df_permits_wind.GenMastrNummer.notna().sum()            # 26545/35227 -> > 2/3 der Einheiten haben eine Genehmigungsnummer\n",
    "# Wie verteilt sich der Betriebsstatus der Anlagen ohne Genehmigung?\n",
    "ind = df_permits_wind.GenMastrNummer.isna()\n",
    "df_permits_wind.loc[ind,:].groupby('einheit_betriebsstatus')['einheit_mastr_nummer'].count()\n",
    "\n",
    "ind = df_permits_wind.GenMastrNummer.notna()\n",
    "# 7922 Einheiten ohne verknüpfte Genehmigung (8682) sind in Betrieb\n",
    "\n",
    "# Anzahl verbleibender Genehmigungen\n",
    "len(df_permits_wind.GenMastrNummer[ind].unique())       # 15748\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabelle genehmigungen erstellen\n",
    "# Welches ist die Datengrundlage?\n",
    "# Inhalte/Typen überprüfen\n",
    "# Insert Statement schreiben\n",
    "# Genehmigungsnummer als Primary-key setzen\n",
    "columns_remove = df_permit.columns[[11,14,15,16]]\n",
    "columns_permit = [col for col in df_permit.columns if col not in columns_remove]\n",
    "\n",
    "gen_number = df_permits_wind.GenMastrNummer[df_permits_wind.GenMastrNummer.notna()]\n",
    "df_permit_upload = df_permit[df_permit['GenMastrNummer'].isin(gen_number)]\n",
    "df_permit_upload = df_permit_upload[columns_permit]\n",
    "df_permit_upload.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set schema_name and table_name\n",
    "schema_name = \"public\"\n",
    "table_name = \"permits\"\n",
    "\n",
    "### Get python data types\n",
    "pd_types = [df_permit_upload[col].dtype for col in columns_permit]\n",
    "\n",
    "# dictionary mapping the data types: Postgres Data type = pandas data type\n",
    "map_types = {'bool': 'bool', \n",
    "                 'float8': 'float64', \n",
    "                 'date' : '<M8[ns]', \n",
    "                 'varchar':'O'}\n",
    "\n",
    "pk_column = \"GenMastrNummer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_columns = []\n",
    "for col in columns_permit:\n",
    "    if col not in pk_column:\n",
    "    # SQL create column statement for this column: 'column_name pgsql-type,'. Leave out constraints for now\n",
    "    # to lowercase, underscore at uppercase\n",
    "        name = change_case(col)\n",
    "        sql_type = dtype_sqltype(df_permit_upload[col].dtype, map_types)\n",
    "        sql_columns.append(f\"{name} {sql_type}\")\n",
    "        \n",
    "sql_columns = \", \\n \".join(sql_columns)\n",
    "\n",
    "sql_pk = f\"{change_case(pk_column)} {dtype_sqltype(df_permit_upload[pk_column].dtype, map_types)} PRIMARY KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PK-column\n",
    "# leave out geo-columns -> added later on\n",
    "# add primary key\n",
    "sql_drop = f\"DROP TABLE IF EXISTS {schema_name}.{table_name};\"\n",
    "\n",
    "sql_create = f\"\"\"\n",
    "CREATE TABLE {schema_name}.{table_name} (\n",
    "{sql_pk},\n",
    "{sql_columns} \n",
    ");\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conn)\n",
    "conn_cursor = conn.cursor()\n",
    "\n",
    "# drop table if it already exists\n",
    "conn_cursor.execute(sql_drop)\n",
    "\n",
    "# table creation\n",
    "conn_cursor.execute(sql_create)\n",
    "conn.commit()\n",
    "\n",
    "conn_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Function for one preprocessed row without geodata \n",
    "### to generate column names and values\n",
    "\n",
    "def row_data_to_sql(row_data, columns_data):\n",
    "    ### Lists for column names as needed for the postgres-table and the values as given to the sql statement\n",
    "    columns_sql = []\n",
    "    values_sql = []\n",
    "\n",
    "    for col in columns_data:\n",
    "        val = row_data[col].values[0]\n",
    "        # print(val)\n",
    "        # print(type(val))\n",
    "        \n",
    "    # Test wether the column holds a value and is not empty\n",
    "        if pd.notna(val):\n",
    "        # add column name\n",
    "            columns_sql.append(change_case(col))\n",
    "        # Apply date to string transformation\n",
    "            if isinstance(val, str):\n",
    "                #print(\"is_str\")\n",
    "                values_sql.append(f\"$${val}$$\")   # add a pair of parentheses to keep for the join\n",
    "            elif is_datetime64_any_dtype(val):\n",
    "                #print(\"is_datetime\")\n",
    "                val = np.datetime_as_string(val, unit=\"D\")\n",
    "                values_sql.append(f\"'{val}'\")\n",
    "            else:\n",
    "                #print(\"is_float_bool\")\n",
    "                values_sql.append(str(val))     # cast to str without adding parentheses\n",
    "    \n",
    "    return columns_sql, values_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper function to construct the INSERT query for one row \n",
    "#   from the columns_sql and values_sql lists\n",
    "\n",
    "def join_insert_sql(columns_sql, values_sql):\n",
    "    # join sql-column names and values respectively into a single string\n",
    "    columns_sql = \", \\n \".join(columns_sql) \n",
    "    values_sql = \", \\n \".join(values_sql)\n",
    "\n",
    "    # Create INSERT-Query for one row\n",
    "    sql_insert = f\"\"\"INSERT INTO {schema_name}.{table_name} (\n",
    "        {columns_sql} )\n",
    "    VALUES (\n",
    "        {values_sql}\n",
    "        );\n",
    "    \"\"\"\n",
    "    \n",
    "    return sql_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for all rows\n",
    "def df_to_sql_insert(df_permits_upload, conn_db):\n",
    "    # join sql-column names and values respectively into a single string\n",
    "    ### Loop through columns\n",
    "    ### How should the geo-insert look like\n",
    "    columns_wind = df_permits_upload.columns\n",
    "\n",
    "    ### column names where each name corresponds to one value (not true for db column geom) \n",
    "    columns_data = columns_wind #[col for col in columns_wind if col not in geo_columns]\n",
    "    \n",
    "    ### List to hold all INSERT Statements\n",
    "    inserts_all = []\n",
    "    \n",
    "    for i in range(len(df_permits_upload)):\n",
    "        row_wind = df_permits_upload.iloc[[i],:]\n",
    "        \n",
    "        row_data = row_wind[columns_data]    \n",
    "        #row_geo = row_wind[geo_columns]\n",
    "        \n",
    "        columns_sql, values_sql = row_data_to_sql(row_data, columns_data)\n",
    "        #columns_sql, values_sql = row_geo_to_sql(row_geo, geo_columns, columns_sql, values_sql)\n",
    "        insert_sql = join_insert_sql(columns_sql, values_sql)\n",
    "        \n",
    "        inserts_all.append(insert_sql)\n",
    "    \n",
    "    inserts_all_sql = \" \\n \".join(inserts_all)\n",
    "    \n",
    "        # Establish a connection to the database    \n",
    "    try:\n",
    "        # Create a cursor\n",
    "        cur = conn_db.cursor()\n",
    "    \n",
    "        # Execute your SQL statement\n",
    "        cur.execute(inserts_all_sql)\n",
    "    \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle the exception\n",
    "        print(f\"Error: {e}\")\n",
    "        conn.rollback()\n",
    "    \n",
    "    finally:\n",
    "        # Close the cursor and connection\n",
    "        cur.close()\n",
    "        #conn.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000  # Set the desired batch size\n",
    "total_rows = len(df_permit_upload)\n",
    "\n",
    "for i in range(0, total_rows, batch_size):\n",
    "    df_batch = df_permit_upload[i:i+batch_size]\n",
    "    df_to_sql_insert(df_batch, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gen_mastr_nummer is already within the wind_extended data\n",
    "### Foreign Key constraint needs to be set\n",
    "schema_name = \"public\"\n",
    "table_name = \"wind_extended\"\n",
    "table_name_2 = \"permits\"\n",
    "\n",
    "foreign_key_column = change_case(\"GenMastrNummer\")\n",
    "\n",
    "sql_foreign_key = f\"\"\"\n",
    "    ALTER TABLE {schema_name}.{table_name}\n",
    "    ADD CONSTRAINT foreign_key_{foreign_key_column} \n",
    "    FOREIGN KEY ({foreign_key_column})\n",
    "    REFERENCES {schema_name}.{table_name_2} ({foreign_key_column});\n",
    "    \"\"\"\n",
    "\n",
    "### Not necessary, since information_schema.columns revealed, that the column is already nullable\n",
    "sql_drop_not_null_fk = f\"\"\"\n",
    "    ALTER TABLE {schema_name}.{table_name} \n",
    "    ALTER COLUMN {foreign_key_column} DROP NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "print(conn)\n",
    "conn_cursor = conn.cursor()\n",
    "\n",
    "# Add foreign key constraint\n",
    "conn_cursor.execute(sql_foreign_key)\n",
    "#conn_cursor.execute(sql_drop_not_null_fk)\n",
    "conn.commit()\n",
    "\n",
    "conn_cursor.close()\n",
    "\n",
    "### Foreign-Key column was not explicitly set to null Documentation and Chat-GPT unclear wether this is a good solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check wether the join wind_extended-permits is the same as df_permits_units\n",
    "sql_join = \"\"\"\n",
    "SELECT wind.gen_mastr_nummer as gen_wind,\n",
    "permits.gen_mastr_nummer as gen_permits, \n",
    "wind.einheit_mastr_nummer as einheit_wind \n",
    "FROM public.wind_extended as wind\n",
    "LEFT JOIN public.permits as permits\n",
    "ON wind.gen_mastr_nummer = permits.gen_mastr_nummer\n",
    "ORDER BY gen_wind;\n",
    "\"\"\"  \n",
    "\n",
    "df_join_wind_permits = pd.read_sql(sql_join, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_wind_permits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_permits_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_test = pd.merge(df_join_wind_permits, df_permits_units, \n",
    "                           left_on=\"einheit_wind\", right_on=\"Einheit\",\n",
    "                           how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_wind = gen_permits?\n",
    "# GenMastrNummer == gen_wind?\n",
    "# einheit_wind == Einheit\n",
    "\n",
    "gen_join_test = []\n",
    "gen_join_mastr_test = []\n",
    "gen_join_mastr_einheit_test = []\n",
    "for index, row in df_join_test.iterrows():\n",
    "    gen_join_test.append(row[\"gen_wind\"] == row[\"gen_permits\"])\n",
    "    gen_join_mastr_test.append(row[\"gen_wind\"] == row[\"GenMastrNummer\"])\n",
    "    gen_join_mastr_einheit_test.append(row[\"einheit_wind\"] == row[\"Einheit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_test[\"gen_join_test\"] = gen_join_test\n",
    "df_join_test[\"gen_join_mastr_test\"] = gen_join_mastr_test\n",
    "df_join_test[\"gen_join_mastr_einheit_test\"] = gen_join_mastr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"gen_join_test\",\"gen_join_mastr_test\", \"gen_join_mastr_einheit_test\"]\n",
    "\n",
    "test_sums = {}\n",
    "\n",
    "for col in cols:\n",
    "    test_sums[col] = df_join_test[col].sum()\n",
    "\n",
    "print(test_sums)\n",
    "# 26545 items of gen_wind from wind_extended are the same as from the open_mastr_download -> difference due to those units\n",
    "# without a permission\n",
    "# 26545 items of units from wind_extended are the same as from the open_mastr_download of the permits\n",
    "# The ones without a permission are not included in the permits download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Permits - compare te distribution of those with a gen_mastr_nummer to those without\n",
    "df_wind_date = df_wind[['id', 'einheit_mastr_nummer', 'inbetriebnahmedatum', 'einheit_betriebsstatus', 'gen_mastr_nummer']]\n",
    "\n",
    "df_wind_date[\"hat_gen\"] = df_wind_date.gen_mastr_nummer.notna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
