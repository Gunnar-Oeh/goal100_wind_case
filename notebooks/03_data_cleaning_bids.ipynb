{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download, Inspect and Upload Permit Data\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import pickle of filenames of .xlsx with infos on bids\n",
    "with open(\"../data/mastr_bids/bids_xlsx.pkl\", mode = \"rb\") as pkl_file:\n",
    "    dict_xlsx = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data_Cleaning After 2018 (2 Tables)\n",
    "\n",
    "# 2020-03-01 - Actually two tables with two different Zuschlagsdaten\n",
    "# Gebotsdatum (first line of raw-data/date in table name and dict-key) and Zuschlagsdatum/Bekanntgabedatum (4th line of raw data)\n",
    "\n",
    "# But Not all lines contain the line with the Zuschlagsdatum\n",
    "\n",
    "# Before extracting data from raw format:\n",
    "# Split by Zuschlagsdatum if necessary\n",
    "# extract Zuschlagsdatum from header-info in raw data\n",
    "# Append Zuschlags and Gebotsdatum to the df\n",
    "\n",
    "# in / before third cell\n",
    "\n",
    "# Pattern which starts the row above the actual header, can appear multiple times in the second sheet of the full-data\n",
    "# and thus distinguish tables with two different Zuschlags/Bekanntgabedaten\n",
    "\n",
    "\n",
    "def extract_df_bids_xlsx(bid_date, path_xlsx):\n",
    "    # Pattern for line in raw .xlsx where Info on the Zuschlagsdatum is hidden\n",
    "    pattern_award_date = \"Die Zuschläge gelten eine Woche\"\n",
    "    # pattern for line in raw .xlsx where a header row occurs\n",
    "    pattern_header = \"Name des Bieters\"\n",
    "\n",
    "    # Patterns for Zuschlagsdatum\n",
    "    pattern = r\"(Bekanntgabe am \\d{1,2}.\\d{2}.\\d{4})\"\n",
    "    pattern_2 = r\"(\\d{1,2}.\\d{2}.\\d{4})\"\n",
    "\n",
    "    # Look into xlsx workbook\n",
    "    xlsx_file = openpyxl.load_workbook(path_xlsx)\n",
    "\n",
    "    num_sheets = len(xlsx_file.sheetnames)\n",
    "\n",
    "    # Determine which sheet to read\n",
    "    if num_sheets > 1:   \n",
    "        sheet_to_read = 1\n",
    "    else:\n",
    "        sheet_to_read = 0\n",
    "\n",
    "    # raw .xlsx including messy headers with infos an Gebots and Zuschlagsdaten and obsolete, non tabular lines\n",
    "    raw_df = pd.read_excel(path_xlsx, sheet_name=sheet_to_read, header=None)\n",
    "\n",
    "    # Boolean series to map, in which row the Sentence about Zuschlagsdatum is hidden\n",
    "    bool_series = raw_df[0].str.startswith(pattern_award_date, na= False)\n",
    "\n",
    "    # Index Values to map, in which rows the header of the tables are hidden\n",
    "    header_rows = raw_df[0].eq(pattern_header)\n",
    "    header_rows = header_rows.index[header_rows]\n",
    "\n",
    "    # Also find\n",
    "    if bool_series.any():\n",
    "        ### Extract indices - self referential: left: boolean series. all index values. \n",
    "        # apply boolean series upon the all the indices -> returns indices where == True\n",
    "        ind = bool_series.index[bool_series]\n",
    "\n",
    "        ### Extract dates from these rows as list\n",
    "        award_dates = raw_df[0][ind].str.extract(pattern)[0].str.extract(pattern_2)[0].tolist()\n",
    "\n",
    "        dict_award_dfs = {}\n",
    "        pos = 0\n",
    "        ### mit while pos < len (ind\n",
    "        while pos < (len(ind)-1):\n",
    "\n",
    "            nrows = nrows = header_rows[pos + 1] - 4 - header_rows[pos]\n",
    "            #skipfooter = len(raw_df) - ind[pos+1] - 3\n",
    "\n",
    "            ### Über Index von Index + 2 = header bis Index+1 - 3 daten extrahieren\n",
    "            df_clean = pd.read_excel(path_xlsx, sheet_name=sheet_to_read, header=header_rows[pos], \n",
    "                                     nrows=nrows) \n",
    "                                     #skipfooter=skipfooter)\n",
    "\n",
    "            dict_award_dfs[award_dates[pos]] = df_clean\n",
    "\n",
    "            pos += 1\n",
    "\n",
    "        else:\n",
    "            df_clean = pd.read_excel(path_xlsx, sheet_name=sheet_to_read, header=header_rows[pos])\n",
    "\n",
    "            dict_award_dfs[award_dates[pos]] = df_clean\n",
    "\n",
    "        ### Patch dict_award_dfs together \n",
    "        df_clean_full = pd.DataFrame() \n",
    "\n",
    "        for award_date, df in dict_award_dfs.items():\n",
    "            df[\"Gebotsdatum\"] = [bid_date] * len(df)\n",
    "            df[\"Zuschlagsdatum\"] = [award_date] * len(df)\n",
    "            \n",
    "            if df_clean_full.empty:\n",
    "                df_clean_full = df\n",
    "            else:\n",
    "                df_clean_full = pd.concat([df_clean_full, df], ignore_index=True)\n",
    "\n",
    "        return df_clean_full\n",
    "\n",
    "    # else - No line found with additional Info on Zuschlagsdates. Take the stuff from below and put it here\n",
    "    else: \n",
    "        df_clean = pd.read_excel(path_xlsx, sheet_name=sheet_to_read, header=header_rows[0])\n",
    "        df_clean[\"Gebotsdatum\"] = [bid_date] * len(df_clean)\n",
    "        df_clean[\"Zuschlagsdatum\"] = [None] * len(df_clean)\n",
    "\n",
    "        return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dfs = {}\n",
    "for bid_date, path_xlsx in dict_xlsx.items():\n",
    "    bid_date = datetime.strftime(bid_date, format = \"%Y-%m-%d\")\n",
    "    dict_dfs[bid_date] = extract_df_bids_xlsx(bid_date=bid_date, path_xlsx=path_xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inspect-Data\n",
    "for key, df in dict_dfs.items():\n",
    "    print(key)\n",
    "    print(df)\n",
    "    print(\"\"\"\n",
    "          \n",
    "          =========================================\n",
    "          \n",
    "          \"\"\")\n",
    "\n",
    "### Data looks good. Data complete for tables where Info on Zuschlagsdatum was hidden in additional columns \n",
    "# and also those, where multiple Tables for multiple Zuschlagsdaten where hidden in sheet 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is still messy:\n",
    "\n",
    "#### merged rows in dfs in at least 2018. \n",
    "- the data does not have two sheets with compact and detailed data but only compact data\n",
    "- The feature \"Angegebner Standort der Anlage\" holds the values for BLD, Landkreis, PLZ, Gemeinde, Gemarkung, Flurstück and Mastr Nummer like: \n",
    "\n",
    "    BLD Niedersachsen, Landkreis Salzgitter, Stadt, PLZ 38239, Gemeinde Salzgitter, Gemarkung Barum:\n",
    "    Registernummer A4497640206941: Flur3: 1/1. \n",
    "    BLD Niedersachsen, Landkreis Salzgitter, Stadt, PLZ 38239, Gemeinde Salzgitter, Gemarkung Watenstedt:\n",
    "    Registernummer A9617510206917: Flur5: 1/22. Registernummer A3273890206938: Flur5: 1/23. Registernummer A5669430206922: Flur5: 2/10. \n",
    "\n",
    " Or:\n",
    "\n",
    "    Niedersachsen, Landkreis Stade, PLZ 21698, Gemeinde Brest, Gemarkung Brest:\n",
    "    Flur 2: 66/1; 66/2; 66/3 (SEE919421623876) \n",
    "    Flur 2: 71; 72 (SEE923510311766) \n",
    "    Gemarkung Wohlerst:\n",
    "    Flur 2: 157/5; 5/7; 1/6 (SEE964469396954) \n",
    "    Flur 2: 5/7 (SEE974053806455) \n",
    "    Flur 2: 7/5; 170/5 (SEE968430555418)\n",
    "\n",
    "- Here one Zuschlags-Nr encapsulates several power-units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count nr of columns:\n",
    "bid_date = []\n",
    "ncol = []\n",
    "for key, df in dict_dfs.items():\n",
    "    ncol.append(df.shape[1])\n",
    "    bid_date.append(key)\n",
    "\n",
    "pd.DataFrame({\"bid_date\":bid_date, \"ncol\":ncol})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function: When an item of 'Angegebener Standort der Anlage' is split into multiple groups\n",
    "### of Administrative Infos: Mastr nr, Flur/Flurst, the function extracts the infos from this item seperately:\n",
    "### dictionary of lists where the lists have the length of the nr of mastr_nrs in this split\n",
    "\n",
    "def extract_mastr_nr_location(split_units, split_administrative):\n",
    "    \n",
    "    # Unnecessary headers which are within the item\n",
    "    patterns_rem = [\"Landkreis\", r\"Stadt|kreisfreie Stadt\", \"Gemeinde\", \"PLZ\", \"Gemarkung\"]\n",
    "\n",
    "    # lists of infos to be filled\n",
    "    flur_list = []\n",
    "    mastr_nr = []\n",
    "    bld_list = []\n",
    "    landkreis_list = []\n",
    "    plz_list = []\n",
    "    gemeinde_list = []\n",
    "    gemarkung_list = []\n",
    "\n",
    "    # clear the info from unnecessary headers and further garbage\n",
    "    for pattern in patterns_rem:\n",
    "        split_administrative = re.sub(pattern, \"\", split_administrative)\n",
    "\n",
    "    names_administrative = [info.strip().rstrip(\"\\n\").replace(\"_x000D_\", \"\").rstrip(\":\") for info in split_administrative.split(\", \")]# \n",
    "    \n",
    "    if names_administrative[2] == \"\":\n",
    "        del names_administrative[2] \n",
    "\n",
    "    # loop: through the list with one item of \"Mastr: Flurst\" and append into the corresponding list\n",
    "    # split_administrative is not repetetive, but holds a different information in each item -> name of the administrative unit \n",
    "    # [\"name bundesland\", ... , \"name gemarkung\"]. So these Items are appended repetitively to the corresponding list\n",
    "\n",
    "    for unit in split_units:\n",
    "    # remove leading and trailing spaces and dots\n",
    "        unit = unit.strip(\" \").strip(\".\")\n",
    "        \n",
    "    # Split at first occurence of \": \"\n",
    "        unit_flurst = unit.split(\": \", maxsplit = 1)\n",
    "        flur_list.append(unit_flurst[1].replace(\"_x000D_\\n\", \"\").rstrip(\". \")) #.rstrip(\"\\n\").rstrip(\":_x000D_\").rstrip(\".\"))\n",
    "        mastr_nr.append(unit_flurst[0])\n",
    "    \n",
    "        bld_list.append(names_administrative[0])\n",
    "        landkreis_list.append(names_administrative[1])\n",
    "        plz_list.append(names_administrative[2])\n",
    "        gemeinde_list.append(names_administrative[3])\n",
    "        gemarkung_list.append(names_administrative[4])\n",
    "\n",
    "    dict_row = {'Bundesland':bld_list,\n",
    "       'Landkreis':landkreis_list, \n",
    "       'Postleitzahl':plz_list, \n",
    "       'Gemeinde':gemeinde_list, \n",
    "       'Gemarkung':gemarkung_list,\n",
    "       'Register_Anlagennr':mastr_nr,\n",
    "       'Flur / Flurstück':flur_list}\n",
    "\n",
    "    return dict_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### final function to apply onto the items of \"Angegebener Standort der Anlage\"\n",
    "def extract_info_standort(item_standort, item_zuschlags_nr):\n",
    "    \n",
    "    # First split: Seperate the possibly multiple BLD ... Gemarkung: Regnr Flur: Flurstück into several of these, \n",
    "    # each starting with BLD ... Gemarkung. So that all witihn one item of the first split level are in the same \n",
    "    # administrative borders (bundesland -> gemarkung is the same)\n",
    "    # Each of the these splits can hold multiple units (mastr_nr) with the corresponding Flur/Flurst entry\n",
    "    split_list_top = item_standort.split(\"BLD\")[1:]\n",
    "    \n",
    "    # Extract a dictionary of lists with repetitive administrative names and\n",
    "    # unique units (mastr_nr) and the flur/flurst these are within\n",
    "    # Make a df of these dicts\n",
    "    \n",
    "    # Create empty df first\n",
    "    df_result = pd.DataFrame()\n",
    "    \n",
    "    for split_top in split_list_top:\n",
    "        \n",
    "        # Second split. [0] item administrative info BLD -> Gemarkung\n",
    "        #               [1:] item Regnr Flur/Flurstück  \n",
    "        split_list_admin = split_top.split(\"Registernummer\")\n",
    "\n",
    "        # Lokational info BLD -> Gemarkung\n",
    "        split_administrative = split_list_admin[0]\n",
    "\n",
    "        # [1:] item Regnr Flur/Flurstück\n",
    "        split_units = split_list_admin[1:]\n",
    "        \n",
    "        dict_row = extract_mastr_nr_location(split_units=split_units, \n",
    "                                             split_administrative=split_administrative)\n",
    "        \n",
    "        if df_result.empty:\n",
    "            \n",
    "            df_result = pd.DataFrame(dict_row)\n",
    "            \n",
    "        else:\n",
    "            df_result = pd.concat([df_result, \n",
    "                                   pd.DataFrame(dict_row)], ignore_index=True)       \n",
    "    \n",
    "    df_result[\"Zuschlags-Nr\"] = [item_zuschlags_nr] * len(df_result)\n",
    "    \n",
    "    return df_result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem only present for two dates: \n",
    "# 2018-02-01\t5 columns\n",
    "# 2018-05-01    4 columns\n",
    "\n",
    "df = dict_dfs[\"2018-02-01\"]\n",
    "\n",
    "test_item = df[\"Angegebener Standort der Anlage\"][0]\n",
    "\n",
    "test_zuschlags_nr = df[\"Zuschlags-Nr\"][0]\n",
    "\n",
    "# BLD Niedersachsen, Landkreis Salzgitter, Stadt, PLZ 38239, Gemeinde Salzgitter, Gemarkung Barum:\n",
    "# Registernummer A4497640206941: Flur3: 1/1. \n",
    "# BLD Niedersachsen, Landkreis Salzgitter, Stadt, PLZ 38239, Gemeinde Salzgitter, Gemarkung Watenstedt:\n",
    "# Registernummer A9617510206917: Flur5: 1/22. Registernummer A3273890206938: Flur5: 1/23. Registernummer A5669430206922: Flur5: 2/10.\n",
    "\n",
    "# Can the string be divided by BLD?\n",
    "# If at least the Gemarkung changes (lowest administrational level above flurstück)\n",
    "# the whole sequence of Bundesland\tLandkreis\tPostleitzahl\tGemeinde\tGemarkung\n",
    "# seems to repeat\n",
    "\n",
    "extract_info_standort(test_item, test_zuschlags_nr)\n",
    "\n",
    "del df, test_item, test_zuschlags_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_column = \"Angegebener Standort der Anlage\"\n",
    "bid_nr = \"Zuschlags-Nr\"\n",
    "bid_date = \"2018-02-01\"\n",
    "\n",
    "dict_cleaned_dfs = {}\n",
    "df_messy = dict_dfs[bid_date]\n",
    "\n",
    "# Empty long df to be filled and merged with the remainder of the messy-df\n",
    "df_clean_long = pd.DataFrame()\n",
    "\n",
    "for index, row in df_messy.iterrows():\n",
    "    \n",
    "    df_bid_nr = extract_info_standort(row[messy_column], row[bid_nr])\n",
    "    \n",
    "    if df_clean_long.empty:\n",
    "        df_clean_long = df_bid_nr\n",
    "    else:\n",
    "        df_clean_long = pd.concat([df_clean_long, df_bid_nr], ignore_index=True)\n",
    "\n",
    "### Visually inspected and compared with downloaded .xlsx -> seems fine\n",
    "df_clean_long\n",
    "\n",
    "### Merge with the columns not presend in [df_clean_long\n",
    "cols_keep = [col for col in df_messy.columns if col not in df_clean_long.columns]\n",
    "cols_keep.append(bid_nr)\n",
    "cols_keep.remove('Angegebener Standort der Anlage')\n",
    "\n",
    "dict_cleaned_dfs[bid_date] = pd.merge(df_messy[cols_keep], df_clean_long, on=bid_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cleaned_dfs[bid_date]\n",
    "\n",
    "del cols_keep, df_clean_long, df_bid_nr, df_messy, bid_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dict_dfs[\"2018-05-01\"] - Second Messy-DF\n",
    "bid_date = \"2018-05-01\"\n",
    "\n",
    "df = dict_dfs[bid_date]\n",
    "\n",
    "test_item = df[\"Angegebener Standort der Anlage\"][108]\n",
    "test_zuschlags_nr = df[\"Zuschlags-Nr\"][108]\n",
    "\n",
    "extract_info_standort(test_item, test_zuschlags_nr)\n",
    "\n",
    "# BLD Mecklenburg-Vorpommern, Landkreis Rostock, PLZ 18198, Gemeinde Stäbelow, Gemarkung Bliesekow:_x000D_ \n",
    "# Registernummer A2692250180724: Flur 1: 66. Registernummer A7961750180738: Flur 1: 94.\n",
    "\n",
    "# Function seems to work for this DF too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bid_nr = \"Zuschlags-Nr\"\n",
    "bid_date = \"2018-05-01\"\n",
    "\n",
    "df_messy = dict_dfs[bid_date]\n",
    "\n",
    "# Empty long df to be filled and merged with the remainder of the messy-df\n",
    "df_clean_long = pd.DataFrame()\n",
    "\n",
    "for index, row in df_messy.iterrows():\n",
    "    \n",
    "    df_bid_nr = extract_info_standort(row[messy_column], row[bid_nr])\n",
    "    \n",
    "    if df_clean_long.empty:\n",
    "        df_clean_long = df_bid_nr\n",
    "    else:\n",
    "        df_clean_long = pd.concat([df_clean_long, df_bid_nr], ignore_index=True)\n",
    "        \n",
    "### Visually inspected and compared with downloaded .xlsx -> seems fine\n",
    "df_clean_long\n",
    "\n",
    "### Merge with the columns not presend in [df_clean_long\n",
    "cols_keep = [col for col in df_messy.columns if col not in df_clean_long.columns]\n",
    "cols_keep.append(bid_nr)\n",
    "cols_keep.remove('Angegebener Standort der Anlage')\n",
    "\n",
    "dict_cleaned_dfs[bid_date] = pd.merge(df_messy[cols_keep], df_clean_long, on=bid_nr)\n",
    "\n",
    "dict_cleaned_dfs[bid_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bring the already clean dfs into dict_cleaned_dfs\n",
    "\n",
    "rem_dates = [key for key in dict_dfs.keys() if key not in dict_cleaned_dfs.keys()]\n",
    "\n",
    "for bid_date in rem_dates:\n",
    "    dict_cleaned_dfs[bid_date] = dict_dfs[bid_date]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dict_cleaned_dfs.items():\n",
    "    print(key)\n",
    "    print(len(df.columns))\n",
    "    print(df.columns)\n",
    "    print(\"\"\"\n",
    "          \n",
    "          =========================================\n",
    "          \n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to columns: \"Anlagennummer\" or \"Registernummer\" to \"Register_Anlagennr\"\n",
    "\n",
    "# Set of columns to keep, remove those not within and add empty if not present    \n",
    "columns_all = ['Name des Bieters', 'Gebots-Nr', 'Zuschlags-Nr', 'Bundesland',\n",
    "       'Landkreis', 'Postleitzahl', 'Gemeinde', 'Gemarkung',\n",
    "       'Flur / Flurstück', 'Register_Anlagennr', 'Gebotsdatum',\n",
    "       'Zuschlagsdatum']\n",
    "# Set common column names (12)\n",
    "# remove those not within\n",
    "# add those missing\n",
    "\n",
    "# Add empty column Bundesland to 2022-12-01, 2022-02-01, 2022-05-01\n",
    "# Add Postleitzahl to 2022-02-01, 2022-05-01\n",
    "# remove Kassenzeichen from 2018-02-01\n",
    "# remove Bemerung from 2023-05-01\n",
    "\n",
    "dict_final_dfs = {}\n",
    "\n",
    "for key, df in dict_cleaned_dfs.items():\n",
    "    # Create a copy of the DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Remove dots from column names\n",
    "    for name in df_copy.columns:\n",
    "        df_copy = df_copy.rename(columns={name: name.rstrip(\".\")})\n",
    "    del name\n",
    "\n",
    "    # Unified column for Unit-Nr\n",
    "    for name in [\"Anlagennummer\", \"Registernummer\"]:\n",
    "        if name in df_copy.columns:\n",
    "            df_copy = df_copy.rename(columns={name: \"Register_Anlagennr\"})\n",
    "            break\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    for col in df_copy.columns:\n",
    "        if col not in columns_all:\n",
    "            df_copy.drop(col, axis=1, inplace=True)\n",
    "    del col\n",
    "\n",
    "    # Add missing columns\n",
    "    for col in columns_all:\n",
    "        if col not in df_copy.columns:\n",
    "            df_copy[col] = None\n",
    "\n",
    "    dict_final_dfs[key] = df_copy.reindex(columns=columns_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dict_final_dfs.items():\n",
    "    print(key)\n",
    "    #print(len(df.columns))\n",
    "    print(df.head())\n",
    "    print(\"\"\"\n",
    "          \n",
    "          =========================================\n",
    "          \n",
    "          \"\"\")\n",
    "    \n",
    "# Data looks good -> concat to one df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids = pd.DataFrame()\n",
    "\n",
    "for df in dict_final_dfs.values():\n",
    "    if df_bids.empty:\n",
    "        df_bids = df\n",
    "    else:\n",
    "        df_bids = pd.concat([df_bids, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/mastr_bids/bids_cleaned_2018_2023.pkl\", mode = \"wb\") as pkl_file:\n",
    "    pickle.dump(df_bids, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load dict of 2017s dfs to clean them\n",
    "with open(\"../data/mastr_bids/bids_2017_html.pkl\", mode = \"rb\") as pkl_file:\n",
    "    dict_dfs_2017 = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messy Data in 2017:\n",
    "\n",
    "- No columns/data for Anlagen/Registernr\n",
    "- Data on administrative units and location is similarly structured as above in one cell of \"Standort\":\n",
    "\n",
    "```\n",
    "Landkreis Diepholz, PLZ 27211,                                                     \n",
    "Gemeinde Bassum, Gemarkung Apelstedt: Anlage 1: Flur 6: 025.\n",
    "Gemeinde Bassum, Gemarkung Nienstedt: Anlage 2: Flur 7: 011. Anlage 3: Flur 7: 015/1.\n",
    "Gemeinde Bassum, Gemarkung Schorlingborstel: Anlage 4: Flur 6: 032.\n",
    "```\n",
    "- (BLD) - only present for 2017-11, Landkreis, PLZ as first row in cell, which is not repeated for 2017-08 and 2017-11\n",
    "-> While the first line is missing for the data of may 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(split_units, split_administrative):\n",
    "    # Unnecessary headers which are within the item\n",
    "    patterns_rem = [\"Landkreis\", \"PLZ\", r\"Stadt|kreisfreie Stadt\", \"Gemeinde\", \"Gemarkung\"]\n",
    "\n",
    "    # lists of infos to be filled\n",
    "    \n",
    "    flur_list = []\n",
    "    gemeinde_list = []\n",
    "    gemarkung_list = []\n",
    "\n",
    "    # clear the info from unnecessary headers and further garbage\n",
    "    for pattern in patterns_rem:\n",
    "        split_administrative = re.sub(pattern, \"\", split_administrative)\n",
    "\n",
    "    # List of 2 -> Gemeinde, Gemarkung\n",
    "    names_administrative = [info.strip().rstrip(\":\") for info in split_administrative.split(\", \")]\n",
    "\n",
    "    for unit in split_units:\n",
    "        # remove leading and trailing spaces and dots\n",
    "        unit = unit.strip(\" \").strip(\".\")\n",
    "\n",
    "        # load unit info into the corresponding list\n",
    "        \n",
    "        flur_list.append(unit)\n",
    "        gemeinde_list.append(names_administrative[0])\n",
    "        gemarkung_list.append(names_administrative[1])\n",
    "\n",
    "    # dictionary which holds all splitted values from this cells standort-info\n",
    "    dict_row = {'Gemeinde':gemeinde_list, \n",
    "                'Gemarkung':gemarkung_list,\n",
    "                'Flur / Flurstück':flur_list}\n",
    "\n",
    "    return dict_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### final stripped down function without extraction of Mastr/Anlagennr \n",
    "#   to apply onto the items of \"Angegebener Standort der Anlage\"\n",
    "\n",
    "def extract_info_standort_2(item_standort, item_zuschlags_nr):\n",
    "    \n",
    "    # First split- seperate multiple Geminde, Gemarkung: Anlage ... into several items where each holds all units within the same gemarkung\n",
    "    split_list_top = item_standort.split(\"Gemeinde\")[1:]\n",
    "\n",
    "    # Extract a dictionary of lists with repetitive administrative names and\n",
    "    # unique units (mastr_nr) and the flur/flurst these are within\n",
    "    # Make a df of these dicts\n",
    "    \n",
    "    # Create empty df first\n",
    "    df_result = pd.DataFrame()\n",
    "    \n",
    "    for split_top in split_list_top:\n",
    "        split_list_admin = re.split(r\"Anlage \\d{1,2}:\", split_top)\n",
    "    \n",
    "        # Lokational info Gemeinde -> Gemarkung\n",
    "        split_administrative = split_list_admin[0]\n",
    "    \n",
    "        # [1:] item Flur/Flurstück\n",
    "        split_units = split_list_admin[1:]\n",
    "        \n",
    "        dict_row = extract_location(split_units=split_units, \n",
    "                                             split_administrative=split_administrative)\n",
    "        \n",
    "        if df_result.empty:\n",
    "            \n",
    "            df_result = pd.DataFrame(dict_row)\n",
    "            \n",
    "        else:\n",
    "            df_result = pd.concat([df_result, \n",
    "                                   pd.DataFrame(dict_row)], ignore_index=True)       \n",
    "    \n",
    "    df_result[\"Zuschlags-Nr\"] = [item_zuschlags_nr] * len(df_result)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Messy columns also present in 2017 data\n",
    "df = dict_dfs_2017[\"2017-05-01\"]\n",
    "\n",
    "### Again structured differently - Registernummer is missing - No Landkreis and Bundesland\n",
    "### Units are distinguished by \"Anlage:\"\n",
    "item_standort = df[\"Standort\"][65]\n",
    "item_zuschlags_nr = df[\"Zuschlags-Nr\"][65]\n",
    "\n",
    "print(item_standort)\n",
    "extract_info_standort_2(item_standort=item_standort, item_zuschlags_nr=item_zuschlags_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean for 2017 \n",
    "messy_column = \"Standort\"\n",
    "\n",
    "bid_nr = \"Zuschlags-Nr\"\n",
    "bid_date = \"2017-05-01\"\n",
    "\n",
    "df_messy = dict_dfs_2017[bid_date]\n",
    "\n",
    "# Empty long df to be filled and merged with the remainder of the messy-df\n",
    "df_clean_long = pd.DataFrame()\n",
    "\n",
    "for index, row in df_messy.iterrows():\n",
    "    \n",
    "    df_bid_nr = extract_info_standort_2(row[messy_column], row[bid_nr])\n",
    "    \n",
    "    if df_clean_long.empty:\n",
    "        df_clean_long = df_bid_nr\n",
    "    else:\n",
    "        df_clean_long = pd.concat([df_clean_long, df_bid_nr], ignore_index=True)\n",
    "        \n",
    "### Visually inspected and compared with downloaded .xlsx -> seems fine\n",
    "df_clean_long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge with the columns not presend in [df_clean_long\n",
    "cols_keep = [col for col in df_messy.columns if col not in df_clean_long.columns]\n",
    "\n",
    "cols_keep.append(bid_nr)\n",
    "cols_keep.remove('Standort')\n",
    "\n",
    "dict_cleaned_2017_dfs = {}\n",
    "dict_cleaned_2017_dfs[bid_date] = pd.merge(df_messy[cols_keep], df_clean_long, on=bid_nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017-11 and 2017-08\n",
    "\n",
    "- column for Standort is even more variable in these dfs\n",
    "- function to extract data needs a more dynamic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_standort(item_standort):\n",
    "    # Split item of \"Standort\" into items of either administrative info or unit info on multiple Flur/Flurstück in 1 string\n",
    "    split_list_top = re.split(r\"Anlage \\d{1,2}:\", item_standort)\n",
    "\n",
    "    # Empty\n",
    "    split_2 = []\n",
    "    # Split item of multiple flurstücke into seperated items with one unit respectively\n",
    "    for split_top in split_list_top:\n",
    "        if split_top.startswith(\" Flur\"):\n",
    "            [split_2.append(split) for split in split_top.split(\". \")]\n",
    "        else:\n",
    "            split_2.append(split_top)\n",
    "\n",
    "    # List with [\"Bld -> Gemarkung\", \"Flur...\",]\n",
    "    split_2 = [item.strip() for item in split_2 if item != \"\"]\n",
    "\n",
    "    return split_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def units_into_dict(split_2):\n",
    "    # Sort units into a dictionary.\n",
    "    # keys are the strings of administrative infos\n",
    "    # values are lists of multiple unit items, which lie within the administrative unit defined in the key\n",
    "    current_key = None\n",
    "    current_group = []\n",
    "    dict_admin_units = {}\n",
    "\n",
    "    for item in split_2:\n",
    "        # Check if the item is a Unit Info (\"Flur\") or administrative info for one or several units\n",
    "        if item.startswith(\"Flur\"):\n",
    "            # if it is unit info - append to current group of the same administrative info\n",
    "            current_group.append(item)\n",
    "\n",
    "        # If administrative info?\n",
    "        else:\n",
    "            # administrative info of the first group?\n",
    "            if current_key is not None:\n",
    "                # No -> create key (administrative info) - value (unit location) info pair in dictionary result\n",
    "                dict_admin_units[current_key] = current_group\n",
    "                # empty current group to be refilled\n",
    "                current_group = []\n",
    "                # overwrite current key\n",
    "                current_key = item\n",
    "            else:\n",
    "                # first occurrence of admin-info -> start of list\n",
    "                current_key = item\n",
    "\n",
    "    # loop ended, current key-value pair must be written into dict            \n",
    "    dict_admin_units[current_key] = current_group\n",
    "\n",
    "    return dict_admin_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_admin_unit(administrative_info, split_units):\n",
    "    patterns_rem = [\"Bundesland\", \"Landkreis\", r\"Stadt|kreisfreie Stadt\", \n",
    "                    \"Gemeinde\", r\"[Pp][Ll][Zz]\", \"Postleitzahl\", \"Gemarkung\"]\n",
    "    \n",
    "    split_administrative = administrative_info.split(\", \")\n",
    "    \n",
    "    # clear the info from unnecessary headers and further garbage\n",
    "    for pattern in patterns_rem:\n",
    "        split_administrative = [re.sub(pattern, \"\", level_admin) for level_admin in split_administrative]\n",
    "    \n",
    "    names_administrative = [info.strip().rstrip(\":\") for info in split_administrative]\n",
    "    \n",
    "    len_administrative = len (names_administrative)\n",
    "    len_units = len(split_units)\n",
    "    \n",
    "    # Define the whole possible range of administrative levels\n",
    "    # levels_admin = [\"bundesland_list\", \"landkreis_list\", \"plz_list\", \"gemeinde_list\", \"gemarkung_list\"]\n",
    "    levels_admin = [\"Bundesland\", \"Landkreis\", \"PLZ\", \"Gemeinde\", \"Gemarkung\"]\n",
    "    \n",
    "    # Create dict with an empty list for each level\n",
    "    dict_row = {}\n",
    "    for level in levels_admin:\n",
    "       dict_row[level] = []\n",
    "    del level\n",
    "\n",
    "    flur_list = []\n",
    "    \n",
    "    for unit in split_units:\n",
    "        # remove leading and trailing spaces and dots\n",
    "        unit = unit.strip().strip(\".\")\n",
    "\n",
    "        # load unit info into the corresponding list\n",
    "        flur_list.append(unit)\n",
    "    \n",
    "    # Fill given values for administrative levels    \n",
    "    for i in range(len_administrative):\n",
    "        ind = -(i+1)\n",
    "        level = levels_admin[ind]\n",
    "        dict_row[level] = [names_administrative[ind]] * len_units\n",
    "    \n",
    "    # Fill those not present: levels_admin 1: len(levels_admin) - len_administrative    \n",
    "    levels_missing = len(levels_admin) - len_administrative\n",
    "    \n",
    "    for i in range(levels_missing):\n",
    "        \n",
    "        level = levels_admin[i]\n",
    "        dict_row[level] = [None] * len_units\n",
    "    \n",
    "    dict_row[\"Flur / Flurstück\"] = flur_list\n",
    "\n",
    "    return dict_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for one cell of Standort\n",
    "def extract_info_standort_3(item_standort, item_zuschlags_nr):\n",
    "    split_2 = split_standort(item_standort)\n",
    "    \n",
    "    dict_admin_units = units_into_dict(split_2)\n",
    "    \n",
    "    df_result = pd.DataFrame()\n",
    "    # print(dict_admin_units)\n",
    "    \n",
    "    for administrative_info, split_units in dict_admin_units.items():\n",
    "        dict_row = combine_admin_unit(administrative_info, split_units)\n",
    "        df_row = pd.DataFrame(dict_row)\n",
    "        \n",
    "        if df_result.empty:\n",
    "            df_result = df_row\n",
    "        else:\n",
    "            df_result = pd.concat([df_result, df_row], ignore_index=True)\n",
    "    \n",
    "    df_result[\"Zuschlags-Nr\"] = [item_zuschlags_nr] * len(df_result)\n",
    "    df_result.ffill(inplace=True)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_column = \"Standort\"\n",
    "\n",
    "bid_nr = \"Zuschlags-Nr\"\n",
    "bid_date = \"2017-08-01\"\n",
    "\n",
    "df_messy = dict_dfs_2017[bid_date]\n",
    "\n",
    "# Empty long df to be filled and merged with the remainder of the messy-df\n",
    "list_dfs = []\n",
    "\n",
    "# Index of errorneous rows (again a different formatting)\n",
    "bid_nr_errors = []\n",
    "\n",
    "for index, row in df_messy.iterrows():\n",
    "    \n",
    "    try:\n",
    "        df_bid_nr = extract_info_standort_3(row[messy_column], row[bid_nr])\n",
    "        \n",
    "        list_dfs.append(df_bid_nr)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Row {index} raised error: {e}\")\n",
    "        print(row[messy_column])\n",
    "        \n",
    "        bid_nr_errors.append(df_messy[bid_nr][index]) \n",
    "\n",
    "### Visually inspected and compared with downloaded .xlsx -> seems fine\n",
    "df_clean_long = pd.concat(list_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_long\n",
    "\n",
    "### Still dirty data for some cleaned bid-nrs \n",
    "bid_nr_messy = [\"WIN17-2-209\", \"WIN17-2-177\", \"WIN17-2-202\"]\n",
    "\n",
    "ind = df_clean_long[bid_nr].isin(bid_nr_messy)\n",
    "\n",
    "### Add messy bid nrs to list to later save the messy remaining raw-data\n",
    "[bid_nr_errors.append(i) for i in bid_nr_messy]\n",
    "\n",
    "### Remove rows resulting from messy bid nrs from long df\n",
    "\n",
    "df_clean_long = df_clean_long[~ind]\n",
    "\n",
    "### DF errors into a dict\n",
    "dict_messy_2017_dfs = {}\n",
    "dict_messy_2017_dfs[bid_date] = df_messy[df_messy[bid_nr].isin(bid_nr_errors)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge with the columns not presend in [df_clean_long\n",
    "### rename the columns\n",
    "cols_keep = [col for col in df_messy.columns if col not in df_clean_long.columns]\n",
    "\n",
    "cols_keep.append(bid_nr)\n",
    "cols_keep.remove('Standort')\n",
    "\n",
    "dict_cleaned_2017_dfs[bid_date] = pd.merge(df_messy[cols_keep], df_clean_long, on=bid_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All for 2017-11\n",
    "\n",
    "messy_column = \"Standort\"\n",
    "\n",
    "bid_nr = \"Zuschlags-Nr\"\n",
    "bid_date = \"2017-11-01\"\n",
    "\n",
    "df_messy = dict_dfs_2017[bid_date]\n",
    "\n",
    "# Empty long df to be filled and merged with the remainder of the messy-df\n",
    "list_dfs = []\n",
    "\n",
    "# Index of errorneous rows (again a different formatting)\n",
    "bid_nr_errors = []\n",
    "\n",
    "for index, row in df_messy.iterrows():\n",
    "    \n",
    "    try:\n",
    "        df_bid_nr = extract_info_standort_3(row[messy_column], row[bid_nr])\n",
    "        \n",
    "        list_dfs.append(df_bid_nr)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Row {index} raised error: {e}\")\n",
    "        print(row[messy_column])\n",
    "        \n",
    "        bid_nr_errors.append(df_messy[bid_nr][index]) \n",
    "\n",
    "### Visually inspected and compared with downloaded .xlsx -> seems fine\n",
    "df_clean_long = pd.concat(list_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"WIN17-3-165\"]\n",
    "\n",
    "ind = df_clean_long[bid_nr].isin(bid_nr_messy)\n",
    "\n",
    "### Add messy bid nrs to list to later save the messy remaining raw-data\n",
    "[bid_nr_errors.append(i) for i in bid_nr_messy]\n",
    "\n",
    "### Remove rows resulting from messy bid nrs from long df\n",
    "\n",
    "df_clean_long = df_clean_long[~ind]\n",
    "\n",
    "### messy remaining df into dict\n",
    "### DF errors into a dict\n",
    "dict_messy_2017_dfs[bid_date] = df_messy[df_messy[bid_nr].isin(bid_nr_errors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge with the columns not presend in [df_clean_long\n",
    "### rename the columns\n",
    "cols_keep = [col for col in df_messy.columns if col not in df_clean_long.columns]\n",
    "\n",
    "cols_keep.append(bid_nr)\n",
    "cols_keep.remove('Standort')\n",
    "\n",
    "dict_cleaned_2017_dfs[bid_date] = pd.merge(df_messy[cols_keep], df_clean_long, on=bid_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bid_date, df in dict_cleaned_2017_dfs.items():\n",
    "    print(bid_date)\n",
    "    print(df.head())\n",
    "    print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to columns: \"Anlagennummer\" or \"Registernummer\" to \"Register_Anlagennr\"\n",
    "\n",
    "# Set of columns to keep, remove those not within and add empty if not present    \n",
    "columns_all = ['Name des Bieters', 'Gebots-Nr', 'Zuschlags-Nr', 'Bundesland',\n",
    "       'Landkreis', 'Postleitzahl', 'Gemeinde', 'Gemarkung',\n",
    "       'Flur / Flurstück', 'Register_Anlagennr', 'Gebotsdatum',\n",
    "       'Zuschlagsdatum']\n",
    "# Set common column names (12)\n",
    "# remove those not within\n",
    "# add those missing\n",
    "\n",
    "dict_final_2017_dfs = {}\n",
    "\n",
    "for key, df in dict_cleaned_2017_dfs.items():\n",
    "    # Create a copy of the DataFrame\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Rename PLZ\n",
    "    df_copy.rename({\"PLZ\": \"Postleitzahl\"}, axis = 1, inplace=True)\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    for col in df_copy.columns:\n",
    "        if col not in columns_all:\n",
    "            df_copy.drop(col, axis=1, inplace=True)\n",
    "    del col\n",
    "\n",
    "    # Add missing columns\n",
    "    for col in columns_all:\n",
    "        if col not in df_copy.columns:\n",
    "            df_copy[col] = None\n",
    "\n",
    "    dict_final_2017_dfs[key] = df_copy.reindex(columns=columns_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bind together and save\n",
    "df_bids_2017 = pd.DataFrame()\n",
    "\n",
    "for df in dict_final_2017_dfs.values():\n",
    "    if df_bids_2017.empty:\n",
    "        df_bids_2017 = df\n",
    "    else:\n",
    "        df_bids_2017 = pd.concat([df_bids_2017, df], ignore_index=True)\n",
    "        \n",
    "df_bids_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bind df_bids_2017 and df_bids (2018-23) together\n",
    "df_bids_all = pd.concat([df_bids, df_bids_2017], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save as pkl\n",
    "with open(\"../data/mastr_bids/bids_cleaned_2017_2023.pkl\", mode = \"wb\") as pkl_file:\n",
    "    pickle.dump(df_bids_all, pkl_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
