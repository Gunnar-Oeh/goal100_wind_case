{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download, Inspect and Upload Permit Data\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import pickle of filenames of .xlsx with infos on bids\n",
    "with open(\"../data/mastr_bids/bids_xlsx.pkl\", mode = \"rb\") as pkl_file:\n",
    "    dict_xlsx = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data_Cleaning After 2018 (2 Tables)\n",
    "\n",
    "# 2020-03-01 - Actually two tables with two different Zuschlagsdaten\n",
    "# Gebotsdatum (first line of raw-data/date in table name and dict-key) and Zuschlagsdatum/Bekanntgabedatum (4th line of raw data)\n",
    "\n",
    "# But Not all lines contain the line with the Zuschlagsdatum\n",
    "\n",
    "# Before extracting data from raw format:\n",
    "# Split by Zuschlagsdatum if necessary\n",
    "# extract Zuschlagsdatum from header-info in raw data\n",
    "# Append Zuschlags and Gebotsdatum to the df\n",
    "\n",
    "# in / before third cell\n",
    "\n",
    "# Pattern which starts the row above the actual header, can appear multiple times in the second sheet of the full-data\n",
    "# and thus distinguish tables with two different Zuschlags/Bekanntgabedaten\n",
    "\n",
    "\n",
    "def extract_df_bids_xlsx(bid_date, path_xlsx):\n",
    "    # Pattern for line in raw .xlsx where Info on the Zuschlagsdatum is hidden\n",
    "    pattern_award_date = \"Die Zuschläge gelten eine Woche\"\n",
    "    # pattern for line in raw .xlsx where a header row occurs\n",
    "    pattern_header = \"Name des Bieters\"\n",
    "\n",
    "    # Patterns for Zuschlagsdatum\n",
    "    pattern = r\"(Bekanntgabe am \\d{1,2}.\\d{2}.\\d{4})\"\n",
    "    pattern_2 = r\"(\\d{1,2}.\\d{2}.\\d{4})\"\n",
    "\n",
    "    # Look into xlsx workbook\n",
    "    xlsx_file = openpyxl.load_workbook(path_xlsx)\n",
    "\n",
    "    num_sheets = len(xlsx_file.sheetnames)\n",
    "\n",
    "    # Determine which sheet to read\n",
    "    if num_sheets > 1:   \n",
    "        sheet_to_read = 1\n",
    "    else:\n",
    "        sheet_to_read = 0\n",
    "\n",
    "    # raw .xlsx including messy headers with infos an Gebots and Zuschlagsdaten and obsolete, non tabular lines\n",
    "    raw_df = pd.read_excel(path_xlsx, sheet_name=sheet_to_read, header=None)\n",
    "\n",
    "    # Boolean series to map, in which row the Sentence about Zuschlagsdatum is hidden\n",
    "    bool_series = raw_df[0].str.startswith(pattern_award_date, na= False)\n",
    "\n",
    "    # Index Values to map, in which rows the header of the tables are hidden\n",
    "    header_rows = raw_df[0].eq(pattern_header)\n",
    "    header_rows = header_rows.index[header_rows]\n",
    "\n",
    "    # Also find\n",
    "    if bool_series.any():\n",
    "        ### Extract indices - self referential: left: boolean series. all index values. \n",
    "        # apply boolean series upon the all the indices -> returns indices where == True\n",
    "        ind = bool_series.index[bool_series]\n",
    "\n",
    "        ### Extract dates from these rows as list\n",
    "        award_dates = raw_df[0][ind].str.extract(pattern)[0].str.extract(pattern_2)[0].tolist()\n",
    "\n",
    "        dict_award_dfs = {}\n",
    "        pos = 0\n",
    "        ### mit while pos < len (ind\n",
    "        while pos < (len(ind)-1):\n",
    "\n",
    "            nrows = nrows = header_rows[pos + 1] - 4 - header_rows[pos]\n",
    "            #skipfooter = len(raw_df) - ind[pos+1] - 3\n",
    "\n",
    "            ### Über Index von Index + 2 = header bis Index+1 - 3 daten extrahieren\n",
    "            df_clean = pd.read_excel(path_xlsx, sheet_name=sheet_to_read, header=header_rows[pos], \n",
    "                                     nrows=nrows) \n",
    "                                     #skipfooter=skipfooter)\n",
    "\n",
    "            dict_award_dfs[award_dates[pos]] = df_clean\n",
    "\n",
    "            pos += 1\n",
    "\n",
    "        else:\n",
    "            df_clean = pd.read_excel(path_xlsx, sheet_name=sheet_to_read, header=header_rows[pos])\n",
    "\n",
    "            dict_award_dfs[award_dates[pos]] = df_clean\n",
    "\n",
    "        ### Patch dict_award_dfs together \n",
    "        df_clean_full = pd.DataFrame() \n",
    "\n",
    "        for award_date, df in dict_award_dfs.items():\n",
    "            df[\"Gebotsdatum\"] = [bid_date] * len(df)\n",
    "            df[\"Zuschlagsdatum\"] = [award_date] * len(df)\n",
    "            \n",
    "            if df_clean_full.empty:\n",
    "                df_clean_full = df\n",
    "            else:\n",
    "                df_clean_full = pd.concat([df_clean_full, df], ignore_index=True)\n",
    "\n",
    "        return df_clean_full\n",
    "\n",
    "    # else - No line found with additional Info on Zuschlagsdates. Take the stuff from below and put it here\n",
    "    else: \n",
    "        df_clean = pd.read_excel(path_xlsx, sheet_name=sheet_to_read, header=header_rows[0])\n",
    "        df_clean[\"Gebotsdatum\"] = [bid_date] * len(df_clean)\n",
    "        df_clean[\"Zuschlagsdatum\"] = [None] * len(df_clean)\n",
    "\n",
    "        return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dfs = {}\n",
    "for bid_date, path_xlsx in dict_xlsx.items():\n",
    "    bid_date = datetime.strftime(bid_date, format = \"%Y-%m-%d\")\n",
    "    dict_dfs[bid_date] = extract_df_bids_xlsx(bid_date=bid_date, path_xlsx=path_xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inspect-Data\n",
    "for key, df in dict_dfs.items():\n",
    "    print(key)\n",
    "    print(df)\n",
    "    print(\"\"\"\n",
    "          \n",
    "          =========================================\n",
    "          \n",
    "          \"\"\")\n",
    "\n",
    "### Data looks good. Data complete for tables where Info on Zuschlagsdatum was hidden in additional columns \n",
    "# and also those, where multiple Tables for multiple Zuschlagsdaten where hidden in sheet 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is still messy:\n",
    "\n",
    "#### merged rows in dfs in at least 2018. \n",
    "- the data does not have two sheets with compact and detailed data but only compact data\n",
    "- The feature \"Angegebner Standort der Anlage\" holds the values for BLD, Landkreis, PLZ, Gemeinde, Gemarkung, Flurstück and Mastr Nummer like: \n",
    "\n",
    "    BLD Niedersachsen, Landkreis Salzgitter, Stadt, PLZ 38239, Gemeinde Salzgitter, Gemarkung Barum:\n",
    "    Registernummer A4497640206941: Flur3: 1/1. \n",
    "    BLD Niedersachsen, Landkreis Salzgitter, Stadt, PLZ 38239, Gemeinde Salzgitter, Gemarkung Watenstedt:\n",
    "    Registernummer A9617510206917: Flur5: 1/22. Registernummer A3273890206938: Flur5: 1/23. Registernummer A5669430206922: Flur5: 2/10. \n",
    "\n",
    " Or:\n",
    "\n",
    "    Niedersachsen, Landkreis Stade, PLZ 21698, Gemeinde Brest, Gemarkung Brest:\n",
    "    Flur 2: 66/1; 66/2; 66/3 (SEE919421623876) \n",
    "    Flur 2: 71; 72 (SEE923510311766) \n",
    "    Gemarkung Wohlerst:\n",
    "    Flur 2: 157/5; 5/7; 1/6 (SEE964469396954) \n",
    "    Flur 2: 5/7 (SEE974053806455) \n",
    "    Flur 2: 7/5; 170/5 (SEE968430555418)\n",
    "\n",
    "- Here one Zuschlags-Nr encapsulates several power-units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count nr of columns:\n",
    "bid_date = []\n",
    "ncol = []\n",
    "for key, df in dict_dfs.items():\n",
    "    ncol.append(df.shape[1])\n",
    "    bid_date.append(key)\n",
    "\n",
    "pd.DataFrame({\"bid_date\":bid_date, \"ncol\":ncol})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function: When an item of 'Angegebener Standort der Anlage' is split into multiple groups\n",
    "### of Administrative Infos: Mastr nr, Flur/Flurst, the function extracts the infos from this item seperately:\n",
    "### dictionary of lists where the lists have the length of the nr of mastr_nrs in this split\n",
    "\n",
    "def extract_mastr_nr_location(split_units, split_administrative):\n",
    "    \n",
    "    # Unnecessary headers which are within the item\n",
    "    patterns_rem = [\"Landkreis\", r\"Stadt|kreisfreie Stadt\", \"Gemeinde\", \"PLZ\", \"Gemarkung\"]\n",
    "\n",
    "    # lists of infos to be filled\n",
    "    flur_list = []\n",
    "    mastr_nr = []\n",
    "    bld_list = []\n",
    "    landkreis_list = []\n",
    "    plz_list = []\n",
    "    gemeinde_list = []\n",
    "    gemarkung_list = []\n",
    "\n",
    "    # clear the info from unnecessary headers and further garbage\n",
    "    for pattern in patterns_rem:\n",
    "        split_administrative = re.sub(pattern, \"\", split_administrative)\n",
    "\n",
    "    names_administrative = [info.strip().rstrip(\"\\n\").replace(\"_x000D_\", \"\").rstrip(\":\") for info in split_administrative.split(\", \")]# \n",
    "    \n",
    "    if names_administrative[2] == \"\":\n",
    "        del names_administrative[2] \n",
    "\n",
    "    # loop: through the list with one item of \"Mastr: Flurst\" and append into the corresponding list\n",
    "    # split_administrative is not repetetive, but holds a different information in each item -> name of the administrative unit \n",
    "    # [\"name bundesland\", ... , \"name gemarkung\"]. So these Items are appended repetitively to the corresponding list\n",
    "\n",
    "    for unit in split_units:\n",
    "    # remove leading and trailing spaces and dots\n",
    "        unit = unit.strip(\" \").strip(\".\")\n",
    "        \n",
    "    # Split at first occurence of \": \"\n",
    "        unit_flurst = unit.split(\": \", maxsplit = 1)\n",
    "        flur_list.append(unit_flurst[1].replace(\"_x000D_\\n\", \"\").rstrip(\". \")) #.rstrip(\"\\n\").rstrip(\":_x000D_\").rstrip(\".\"))\n",
    "        mastr_nr.append(unit_flurst[0])\n",
    "    \n",
    "        bld_list.append(names_administrative[0])\n",
    "        landkreis_list.append(names_administrative[1])\n",
    "        plz_list.append(names_administrative[2])\n",
    "        gemeinde_list.append(names_administrative[3])\n",
    "        gemarkung_list.append(names_administrative[4])\n",
    "\n",
    "    dict_row = {'Bundesland':bld_list,\n",
    "       'Landkreis':landkreis_list, \n",
    "       'Postleitzahl':plz_list, \n",
    "       'Gemeinde':gemeinde_list, \n",
    "       'Gemarkung':gemarkung_list,\n",
    "       'Register_Anlagennr':mastr_nr,\n",
    "       'Flur / Flurstück':flur_list}\n",
    "\n",
    "    return dict_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### final function to apply onto the items of \"Angegebener Standort der Anlage\"\n",
    "def extract_info_standort(item_standort, item_zuschlags_nr):\n",
    "    \n",
    "    # First split: Seperate the possibly multiple BLD ... Gemarkung: Regnr Flur: Flurstück into several of these, \n",
    "    # each starting with BLD ... Gemarkung. So that all witihn one item of the first split level are in the same \n",
    "    # administrative borders (bundesland -> gemarkung is the same)\n",
    "    # Each of the these splits can hold multiple units (mastr_nr) with the corresponding Flur/Flurst entry\n",
    "    split_list_top = item_standort.split(\"BLD\")[1:]\n",
    "    \n",
    "    # Extract a dictionary of lists with repetitive administrative names and\n",
    "    # unique units (mastr_nr) and the flur/flurst these are within\n",
    "    # Make a df of these dicts\n",
    "    \n",
    "    # Create empty df first\n",
    "    df_result = pd.DataFrame()\n",
    "    \n",
    "    for split_top in split_list_top:\n",
    "        \n",
    "        # Second split. [0] item administrative info BLD -> Gemarkung\n",
    "        #               [1:] item Regnr Flur/Flurstück  \n",
    "        split_list_admin = split_top.split(\"Registernummer\")\n",
    "\n",
    "        # Lokational info BLD -> Gemarkung\n",
    "        split_administrative = split_list_admin[0]\n",
    "\n",
    "        # [1:] item Regnr Flur/Flurstück\n",
    "        split_units = split_list_admin[1:]\n",
    "        \n",
    "        dict_row = extract_mastr_nr_location(split_units=split_units, \n",
    "                                             split_administrative=split_administrative)\n",
    "        \n",
    "        if df_result.empty:\n",
    "            \n",
    "            df_result = pd.DataFrame(dict_row)\n",
    "            \n",
    "        else:\n",
    "            df_result = pd.concat([df_result, \n",
    "                                   pd.DataFrame(dict_row)], ignore_index=True)       \n",
    "    \n",
    "    df_result[\"Zuschlags-Nr\"] = [item_zuschlags_nr] * len(df_result)\n",
    "    \n",
    "    return df_result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem only present for two dates: \n",
    "# 2018-02-01\t5 columns\n",
    "# 2018-05-01    4 columns\n",
    "\n",
    "df = dict_dfs[\"2018-02-01\"]\n",
    "\n",
    "test_item = df[\"Angegebener Standort der Anlage\"][0]\n",
    "\n",
    "test_zuschlags_nr = df[\"Zuschlags-Nr\"][0]\n",
    "\n",
    "# BLD Niedersachsen, Landkreis Salzgitter, Stadt, PLZ 38239, Gemeinde Salzgitter, Gemarkung Barum:\n",
    "# Registernummer A4497640206941: Flur3: 1/1. \n",
    "# BLD Niedersachsen, Landkreis Salzgitter, Stadt, PLZ 38239, Gemeinde Salzgitter, Gemarkung Watenstedt:\n",
    "# Registernummer A9617510206917: Flur5: 1/22. Registernummer A3273890206938: Flur5: 1/23. Registernummer A5669430206922: Flur5: 2/10.\n",
    "\n",
    "# Can the string be divided by BLD?\n",
    "# If at least the Gemarkung changes (lowest administrational level above flurstück)\n",
    "# the whole sequence of Bundesland\tLandkreis\tPostleitzahl\tGemeinde\tGemarkung\n",
    "# seems to repeat\n",
    "\n",
    "extract_info_standort(test_item, test_zuschlags_nr)\n",
    "\n",
    "del df, test_item, test_zuschlags_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_column = \"Angegebener Standort der Anlage\"\n",
    "bid_nr = \"Zuschlags-Nr\"\n",
    "bid_date = \"2018-02-01\"\n",
    "\n",
    "dict_cleaned_dfs = {}\n",
    "df_messy = dict_dfs[bid_date]\n",
    "\n",
    "# Empty long df to be filled and merged with the remainder of the messy-df\n",
    "df_clean_long = pd.DataFrame()\n",
    "\n",
    "for index, row in df_messy.iterrows():\n",
    "    \n",
    "    df_bid_nr = extract_info_standort(row[messy_column], row[bid_nr])\n",
    "    \n",
    "    if df_clean_long.empty:\n",
    "        df_clean_long = df_bid_nr\n",
    "    else:\n",
    "        df_clean_long = pd.concat([df_clean_long, df_bid_nr], ignore_index=True)\n",
    "\n",
    "### Visually inspected and compared with downloaded .xlsx -> seems fine\n",
    "df_clean_long\n",
    "\n",
    "### Merge with the columns not presend in [df_clean_long\n",
    "cols_keep = [col for col in df_messy.columns if col not in df_clean_long.columns]\n",
    "cols_keep.append(bid_nr)\n",
    "cols_keep.remove('Angegebener Standort der Anlage')\n",
    "\n",
    "dict_cleaned_dfs[bid_date] = pd.merge(df_messy[cols_keep], df_clean_long, on=bid_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cleaned_dfs[bid_date]\n",
    "\n",
    "del cols_keep, df_clean_long, df_bid_nr, df_messy, bid_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dict_dfs[\"2018-05-01\"] - Second Messy-DF\n",
    "bid_date = \"2018-05-01\"\n",
    "\n",
    "df = dict_dfs[bid_date]\n",
    "\n",
    "test_item = df[\"Angegebener Standort der Anlage\"][108]\n",
    "test_zuschlags_nr = df[\"Zuschlags-Nr\"][108]\n",
    "\n",
    "extract_info_standort(test_item, test_zuschlags_nr)\n",
    "\n",
    "# BLD Mecklenburg-Vorpommern, Landkreis Rostock, PLZ 18198, Gemeinde Stäbelow, Gemarkung Bliesekow:_x000D_ \n",
    "# Registernummer A2692250180724: Flur 1: 66. Registernummer A7961750180738: Flur 1: 94.\n",
    "\n",
    "# Function seems to work for this DF too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bid_nr = \"Zuschlags-Nr\"\n",
    "bid_date = \"2018-05-01\"\n",
    "\n",
    "df_messy = dict_dfs[bid_date]\n",
    "\n",
    "# Empty long df to be filled and merged with the remainder of the messy-df\n",
    "df_clean_long = pd.DataFrame()\n",
    "\n",
    "for index, row in df_messy.iterrows():\n",
    "    \n",
    "    df_bid_nr = extract_info_standort(row[messy_column], row[bid_nr])\n",
    "    \n",
    "    if df_clean_long.empty:\n",
    "        df_clean_long = df_bid_nr\n",
    "    else:\n",
    "        df_clean_long = pd.concat([df_clean_long, df_bid_nr], ignore_index=True)\n",
    "        \n",
    "### Visually inspected and compared with downloaded .xlsx -> seems fine\n",
    "df_clean_long\n",
    "\n",
    "### Merge with the columns not presend in [df_clean_long\n",
    "cols_keep = [col for col in df_messy.columns if col not in df_clean_long.columns]\n",
    "cols_keep.append(bid_nr)\n",
    "cols_keep.remove('Angegebener Standort der Anlage')\n",
    "\n",
    "dict_cleaned_dfs[bid_date] = pd.merge(df_messy[cols_keep], df_clean_long, on=bid_nr)\n",
    "\n",
    "dict_cleaned_dfs[bid_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bring the already clean dfs into dict_cleaned_dfs\n",
    "\n",
    "rem_dates = [key for key in dict_dfs.keys() if key not in dict_cleaned_dfs.keys()]\n",
    "\n",
    "for bid_date in rem_dates:\n",
    "    dict_cleaned_dfs[bid_date] = dict_dfs[bid_date]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dict_cleaned_dfs.items():\n",
    "    print(key)\n",
    "    print(len(df.columns))\n",
    "    print(df.columns)\n",
    "    print(\"\"\"\n",
    "          \n",
    "          =========================================\n",
    "          \n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to columns: \"Anlagennummer\" or \"Registernummer\" to \"Register_Anlagennr\"\n",
    "\n",
    "# Set of columns to keep, remove those not within and add empty if not present    \n",
    "columns_all = ['Name des Bieters', 'Gebots-Nr', 'Zuschlags-Nr', 'Bundesland',\n",
    "       'Landkreis', 'Postleitzahl', 'Gemeinde', 'Gemarkung',\n",
    "       'Flur / Flurstück', 'Register_Anlagennr', 'Gebotsdatum',\n",
    "       'Zuschlagsdatum']\n",
    "# Set common column names (12)\n",
    "# remove those not within\n",
    "# add those missing\n",
    "\n",
    "# Add empty column Bundesland to 2022-12-01, 2022-02-01, 2022-05-01\n",
    "# Add Postleitzahl to 2022-02-01, 2022-05-01\n",
    "# remove Kassenzeichen from 2018-02-01\n",
    "# remove Bemerung from 2023-05-01\n",
    "\n",
    "dict_final_dfs = {}\n",
    "\n",
    "for key, df in dict_cleaned_dfs.items():\n",
    "    # Create a copy of the DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Remove dots from column names\n",
    "    for name in df_copy.columns:\n",
    "        df_copy = df_copy.rename(columns={name: name.rstrip(\".\")})\n",
    "    del name\n",
    "\n",
    "    # Unified column for Unit-Nr\n",
    "    for name in [\"Anlagennummer\", \"Registernummer\"]:\n",
    "        if name in df_copy.columns:\n",
    "            df_copy = df_copy.rename(columns={name: \"Register_Anlagennr\"})\n",
    "            break\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    for col in df_copy.columns:\n",
    "        if col not in columns_all:\n",
    "            df_copy.drop(col, axis=1, inplace=True)\n",
    "    del col\n",
    "\n",
    "    # Add missing columns\n",
    "    for col in columns_all:\n",
    "        if col not in df_copy.columns:\n",
    "            df_copy[col] = None\n",
    "\n",
    "    dict_final_dfs[key] = df.reindex(columns=columns_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dict_final_dfs.items():\n",
    "    print(key)\n",
    "    print(len(df.columns))\n",
    "    print(df)\n",
    "    print(\"\"\"\n",
    "          \n",
    "          =========================================\n",
    "          \n",
    "          \"\"\")\n",
    "    \n",
    "# Data looks good -> concat to one df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids = pd.DataFrame()\n",
    "\n",
    "for df in dict_final_dfs.values():\n",
    "    if df_bids.empty:\n",
    "        df_bids = df\n",
    "    else:\n",
    "        df_bids = pd.concat([df_bids, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/mastr_bids/bids_cleaned_2018_2023.pkl\", mode = \"wb\") as pkl_file:\n",
    "    pickle.dump(df_bids, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load dict of 2017s dfs to clean them\n",
    "with open(\"../data/mastr_bids/bids_2017_html.pkl\", mode = \"rb\") as pkl_file:\n",
    "    dict_dfs_2017 = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Messy columns also present in 2017 data\n",
    "df = dict_dfs_2017[\"2017-05-01\"]\n",
    "\n",
    "### Again structured differently - Registernummer is missing - No Landkreis and Bundesland\n",
    "### Units are distinguished by \"Anlage:\"\n",
    "item_standort = df[\"Standort\"][65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split- seperate multiple Geminde, Gemarkung: Anlage ... into several items where each holds all units within the same gemarkung\n",
    "split_list_top = item_standort.split(\"Gemeinde\")[1:]\n",
    "\n",
    "# Extract a dictionary of lists with repetitive administrative names and\n",
    "    # unique units (mastr_nr) and the flur/flurst these are within\n",
    "    # Make a df of these dicts\n",
    "    \n",
    "    # Create empty df first\n",
    "df_result = pd.DataFrame()\n",
    "    \n",
    "for split_top in split_list_top:\n",
    "    split_list_admin = re.split(r\"Anlage \\d{1,2}:\", split_top)\n",
    "    \n",
    "    # Lokational info Gemeinde -> Gemarkung\n",
    "    split_administrative = split_list_admin[0]\n",
    "    \n",
    "    # [1:] item Flur/Flurstück\n",
    "    split_units = split_list_admin[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Stemwede,  Oppendorf: '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_administrative\n",
    "\n",
    "# Unnecessary headers which are within the item\n",
    "patterns_rem = [r\"Stadt|kreisfreie Stadt\", \"Gemeinde\", \"Gemarkung\"]\n",
    "\n",
    "# lists of infos to be filled\n",
    "flur_list = []\n",
    "gemeinde_list = []\n",
    "gemarkung_list = []\n",
    "\n",
    "# clear the info from unnecessary headers and further garbage\n",
    "for pattern in patterns_rem:\n",
    "    split_administrative = re.sub(pattern, \"\", split_administrative)\n",
    "\n",
    "names_administrative = [info.strip().rstrip(\":\") for info in split_administrative.split(\", \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stemwede', 'Oppendorf']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "names_administrative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
