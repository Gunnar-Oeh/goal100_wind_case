{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_mastr import Mastr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import supabase_py\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to turn CamelCase to snake_case\n",
    "def change_case(str):\n",
    "    # List comprehension, starts with an _ wich is removed by lstrip(\"_\")\n",
    "    # loops through word, if upper, _ first \"_\"+\n",
    "    # and i.lower() as a string method\n",
    "    # just return i else\n",
    "    return ''.join(['_'+i.lower() if i.isupper() \n",
    "               else i for i in str]).lstrip('_')\n",
    "    \n",
    "def dtype_sqltype(str, map_dict):\n",
    "    \n",
    "    # next() jumps through the iterator until a match is found\n",
    "    # with a an iterator generated by the comprehension inside ()\n",
    "    return next((key for key, val in map_dict.items() if val == str), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connection to local mastr-download\n",
    "db = Mastr()\n",
    "\n",
    "### Inspect db\n",
    "conn = db.engine # Connection engine\n",
    "tables = pd.read_sql_query('SELECT name from sqlite_master where type= \"table\";', conn)\n",
    "df_wind = pd.read_sql_table(\"wind_extended\", conn)\n",
    "columns_wind = list(df_wind.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connection to remote supabase-postgres goal:100 db\n",
    "### Connect to the database\n",
    "# downloaded certiticate\n",
    "# Set connection details in .env\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get connection parameters from environment variables\n",
    "dbname = os.getenv(\"DB_NAME\")\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")\n",
    "ssl_cert_path = os.getenv(\"SSL_CERT_PATH\")\n",
    "\n",
    "# Construct the connection string\n",
    "conn_str = f\"dbname={dbname} user={user} password={password} host={host} port={port} sslmode=require sslrootcert={ssl_cert_path}\"\n",
    "\n",
    "# Etablish connection object\n",
    "\n",
    "#conn.close()\n",
    "conn = psycopg2.connect(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set schema_name and table_name\n",
    "schema_name = \"public\"\n",
    "table_name = \"wind_extended\"\n",
    "\n",
    "### Columns names of local mastr-download\n",
    "columns_wind_om = columns_wind\n",
    "\n",
    "### Get column names of remote db\n",
    "\n",
    "\n",
    "sql_cols = \"\"\"SELECT column_name FROM information_schema.columns WHERE table_name = %s\n",
    "AND table_schema = %s;\"\"\"\n",
    "\n",
    "conn_cursor = conn.cursor()\n",
    "\n",
    "# drop table if it already exists\n",
    "conn_cursor.execute(sql_cols, (table_name, schema_name))\n",
    "columns_wind_rem = conn_cursor.fetchall()\n",
    "\n",
    "conn_cursor.close()\n",
    "\n",
    "columns_wind_rem = [col[0] for col in columns_wind_rem]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reduced column_names before upload as 01_open-mastr_extract suggests\n",
    "columns_remove = [\"Lage\",\t\"Seelage\",\t\"ClusterOstsee\", \"ClusterNordsee\", \"Strasse\", \"StrasseNichtGefunden\",\n",
    "       \"Hausnummer\", \"HausnummerNichtGefunden\", \"Adresszusatz\", \"NetzbetreiberpruefungStatus\", \n",
    "       \"NetzbetreiberpruefungDatum\", \"Wassertiefe\", \"Kuestenentfernung\", \"UtmZonenwert\",\"UtmEast\", \"UtmNorth\",\n",
    " \"GaussKruegerHoch\", \"GaussKruegerRechts\", \"DatenQuelle\", \"DatumDownload\"]\n",
    "columns_wind_om2 = [col for col in columns_wind if col not in columns_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Columns of the open_mastr download not in the database?\n",
    "columns_om_only = [col for col in columns_wind if change_case(col) not in columns_wind_rem]\n",
    "columns_om_only\n",
    "\n",
    "### Is there a column which quickly identifies the offshore vs onshore\n",
    "df_off_on = df_wind[['Lage',\n",
    " 'Seelage',\n",
    " 'ClusterOstsee',\n",
    " 'ClusterNordsee',\n",
    " 'Wassertiefe',\n",
    " 'Kuestenentfernung','Bundesland','Landkreis']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_off_on.info() ### only three entries without an Land/ auf See entry\n",
    "\n",
    "### Seelage: eindeutiger. Lage tuts auch\n",
    "df_off_on.Seelage.unique()\n",
    "\n",
    "df_off = df_off_on[df_off_on.Seelage.isin(['Nordsee', 'Ostsee'])]\n",
    "\n",
    "df_off.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SQL Alter Statement\n",
    "columns_add = [\"Lage\", \"Seelage\"]\n",
    "\n",
    "pd_types = [df_wind[col].dtype for col in columns_add]\n",
    "\n",
    "# dictionary mapping the data types: Postgres Data type = pandas data type\n",
    "map_types = {'bool': 'bool', \n",
    "                 'float8': 'float64', \n",
    "                 'date' : '<M8[ns]', \n",
    "                 'varchar':'O'}\n",
    "\n",
    "### SQL statement to add empty columns and data-type\n",
    "### SQL Inserts with where einheit_mastr_nummer = ...\n",
    "sql_columns = []\n",
    "for col in columns_add:\n",
    "    # SQL create column statement for this column: 'column_name pgsql-type,'. Leave out constraints for now\n",
    "    # to lowercase, underscore at uppercase\n",
    "    name = change_case(col)\n",
    "    sql_type = dtype_sqltype(df_wind[col].dtype, map_types)\n",
    "    sql_columns.append(f\"ADD COLUMN {name} {sql_type}\")\n",
    "\n",
    "sql_columns = \", \\n \".join(sql_columns)\n",
    "\n",
    "sql_alter = f\"\"\"ALTER TABLE {schema_name}.{table_name}\n",
    "{sql_columns};\n",
    "\"\"\"\n",
    "\n",
    "conn_cursor = conn.cursor()\n",
    "\n",
    "# Add columns\n",
    "conn_cursor.execute(sql_alter)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Helper Function for one preprocessed row without geodata \n",
    "### to generate column names and values\n",
    "\n",
    "def row_data_to_sql(row_data, columns_data):\n",
    "    ### Lists for column names as needed for the postgres-table and the values as given to the sql statement\n",
    "    columns_sql = []\n",
    "    values_sql = []\n",
    "\n",
    "    for col in columns_data:\n",
    "        val = row_data[col].values[0]\n",
    "        # print(val)\n",
    "        # print(type(val))\n",
    "        \n",
    "    # Test wether the column holds a value and is not empty\n",
    "        if pd.notna(val):\n",
    "        # add column name\n",
    "            columns_sql.append(change_case(col))\n",
    "        # Apply date to string transformation\n",
    "            if isinstance(val, str):\n",
    "                #print(\"is_str\")\n",
    "                values_sql.append(f\"$${val}$$\")   # add a pair of parentheses to keep for the join\n",
    "            elif is_datetime64_any_dtype(val):\n",
    "                #print(\"is_datetime\")\n",
    "                val = np.datetime_as_string(val, unit=\"D\")\n",
    "                values_sql.append(f\"'{val}'\")\n",
    "            else:\n",
    "                #print(\"is_float_bool\")\n",
    "                values_sql.append(str(val))     # cast to str without adding parentheses\n",
    "    \n",
    "    return columns_sql, values_sql\n",
    "\n",
    "### helper function to construct the INSERT query for one row \n",
    "#   from the columns_sql and values_sql lists\n",
    "\n",
    "def join_insert_sql(columns_sql, values_sql):\n",
    "    # join sql-column names and values respectively into a single string\n",
    "    columns_sql = \", \\n \".join(columns_sql) \n",
    "    values_sql = \", \\n \".join(values_sql)\n",
    "\n",
    "    # Create INSERT-Query for one row\n",
    "    sql_insert = f\"\"\"INSERT INTO {schema_name}.{table_name} (\n",
    "        {columns_sql} )\n",
    "    VALUES (\n",
    "        {values_sql}\n",
    "        )\n",
    "    WHERE {id_columnn} = {id_value}    ;\n",
    "    \"\"\"\n",
    "    \n",
    "    return sql_insert\n",
    "\n",
    "### Function for all rows\n",
    "def df_to_sql_insert(df_upload, conn_db):\n",
    "    # join sql-column names and values respectively into a single string\n",
    "    ### Loop through columns\n",
    "    ### How should the geo-insert look like\n",
    "    columns_wind = df_upload.columns\n",
    "\n",
    "    ### column names where each name corresponds to one value (not true for db column geom) \n",
    "    columns_data = columns_wind #[col for col in columns_wind if col not in geo_columns]\n",
    "    \n",
    "    ### List to hold all INSERT Statements\n",
    "    inserts_all = []\n",
    "    \n",
    "    for i in range(len(df_upload)):\n",
    "        row_wind = df_upload.iloc[[i],:]\n",
    "        \n",
    "        row_data = row_wind[columns_data]    \n",
    "        #row_geo = row_wind[geo_columns]\n",
    "        \n",
    "        columns_sql, values_sql = row_data_to_sql(row_data, columns_data)\n",
    "        #columns_sql, values_sql = row_geo_to_sql(row_geo, geo_columns, columns_sql, values_sql)\n",
    "        insert_sql = join_insert_sql(columns_sql, values_sql)\n",
    "        \n",
    "        inserts_all.append(insert_sql)\n",
    "    \n",
    "    inserts_all_sql = \" \\n \".join(inserts_all)\n",
    "    \n",
    "        # Establish a connection to the database    \n",
    "    try:\n",
    "        # Create a cursor\n",
    "        cur = conn_db.cursor()\n",
    "    \n",
    "        # Execute your SQL statement\n",
    "        cur.execute(inserts_all_sql)\n",
    "    \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle the exception\n",
    "        print(f\"Error: {e}\")\n",
    "        conn.rollback()\n",
    "    \n",
    "    finally:\n",
    "        # Close the cursor and connection\n",
    "        cur.close()\n",
    "        #conn.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
